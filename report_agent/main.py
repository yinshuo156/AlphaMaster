#!/usr/bin/env python3
"""
Report Agent ä¸»å…¥å£è„šæœ¬
ç”¨äºä»å‘½ä»¤è¡Œç”ŸæˆAlphaå› å­åˆ†ææŠ¥å‘Š
"""

import os
import sys
import json
import logging
import argparse
from pathlib import Path
from typing import Optional

# å¯¼å…¥æŠ¥å‘Šç”Ÿæˆå™¨
try:
    from report_generator import ReportGenerator
except ImportError:
    # å¦‚æœç›´æ¥è¿è¡Œè„šæœ¬ï¼Œå°è¯•æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from report_generator import ReportGenerator

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('report_agent.main')


def parse_arguments() -> argparse.Namespace:
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    """
    parser = argparse.ArgumentParser(
        description='Alphaå› å­åˆ†ææŠ¥å‘Šç”Ÿæˆå·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
        ç¤ºä¾‹:
          python main.py --input input_data.json --output report.md --format markdown
          python main.py --input input_data.json --output report.pdf --format pdf
          python main.py --input input_data.json --output report.md --provider openai --model gpt-4 --temperature 0.1
          python main.py --input input_data.json --output report.md --provider dashscope --model qwen-plus
          python main.py --input input_data.json --output report.md --provider deepseek --model deepseek-chat
        """
    )
    
    # å¿…éœ€å‚æ•°
    parser.add_argument(
        '--input', '-i',
        type=str,
        required=True,
        help='è¾“å…¥JSONæ•°æ®æ–‡ä»¶è·¯å¾„'
    )
    
    # å¯é€‰å‚æ•°
    parser.add_argument(
        '--output', '-o',
        type=str,
        help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸è¾“å…¥æ–‡ä»¶åŒåä½†æ‰©å±•åä¸åŒ'
    )
    
    parser.add_argument(
        '--format', '-f',
        type=str,
        choices=['markdown', 'pdf'],
        default='markdown',
        help='è¾“å‡ºæŠ¥å‘Šæ ¼å¼ï¼Œæ”¯æŒmarkdownæˆ–pdfï¼Œé»˜è®¤markdown'
    )
    
    parser.add_argument(
        '--config', '-c',
        type=str,
        help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ŒåŒ…å«LLMè®¾ç½®ç­‰ï¼ˆé…ç½®æ¨¡æ¿ï¼šconfig_template.jsonï¼‰'
    )
    
    # LLMç›¸å…³å‚æ•°
    parser.add_argument(
        '--provider',
        type=str,
        choices=['openai', 'dashscope', 'deepseek'],
        help='LLMæä¾›å•†ï¼Œæ”¯æŒopenaiã€dashscope(é˜¿é‡Œç™¾ç‚¼)ã€deepseek'
    )
    
    parser.add_argument(
        '--model',
        type=str,
        help='LLMæ¨¡å‹åç§°ï¼Œå¦‚gpt-4'
    )
    
    parser.add_argument(
        '--api-key',
        type=str,
        help='LLM APIå¯†é’¥'
    )
    
    parser.add_argument(
        '--base-url',
        type=str,
        help='LLM APIåŸºç¡€URL'
    )
    
    parser.add_argument(
        '--temperature',
        type=float,
        help='LLMæ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§'
    )
    
    parser.add_argument(
        '--max-tokens',
        type=int,
        help='LLMæœ€å¤§tokenæ•°'
    )
    
    parser.add_argument(
        '--debug',
        action='store_true',
        help='å¯ç”¨è°ƒè¯•æ—¥å¿—'
    )
    
    return parser.parse_args()


def load_config(config_file: Optional[str]) -> dict:
    """
    åŠ è½½é…ç½®æ–‡ä»¶
    
    Args:
        config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        é…ç½®å­—å…¸
    """
    config = {}
    
    if config_file and os.path.exists(config_file):
        logger.info(f"ğŸ“ åŠ è½½é…ç½®æ–‡ä»¶: {config_file}")
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logger.info("âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
            raise Exception(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    return config


def build_llm_config(config: dict, args: argparse.Namespace) -> dict:
    """
    æ„å»ºLLMé…ç½®å­—å…¸
    
    Args:
        config: ä»é…ç½®æ–‡ä»¶åŠ è½½çš„é…ç½®
        args: å‘½ä»¤è¡Œå‚æ•°
        
    Returns:
        LLMé…ç½®å­—å…¸
    """
    llm_config = config.get('llm', {})
    
    # å‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆçº§é«˜äºé…ç½®æ–‡ä»¶
    if args.provider:
        llm_config['provider'] = args.provider
    if args.model:
        llm_config['model_name'] = args.model
    if args.api_key:
        llm_config['api_key'] = args.api_key
    if args.base_url:
        llm_config['base_url'] = args.base_url
    if args.temperature is not None:
        llm_config['temperature'] = args.temperature
    if args.max_tokens is not None:
        llm_config['max_tokens'] = args.max_tokens
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šproviderï¼Œé»˜è®¤ä¸ºopenai
    if 'provider' not in llm_config:
        llm_config['provider'] = 'openai'
    
    return llm_config


def determine_output_path(input_path: str, output_path: Optional[str], format_type: str) -> str:
    """
    ç¡®å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„
    
    Args:
        input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
        output_path: æŒ‡å®šçš„è¾“å‡ºæ–‡ä»¶è·¯å¾„
        format_type: è¾“å‡ºæ ¼å¼
        
    Returns:
        è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    if output_path:
        return output_path
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºè·¯å¾„ï¼Œä½¿ç”¨è¾“å…¥æ–‡ä»¶åä½†æ›´æ”¹æ‰©å±•å
    input_name = os.path.splitext(os.path.basename(input_path))[0]
    extension = 'md' if format_type == 'markdown' else 'pdf'
    
    # è¾“å‡ºæ–‡ä»¶æ”¾åœ¨è¾“å…¥æ–‡ä»¶çš„åŒä¸€ç›®å½•
    input_dir = os.path.dirname(input_path) or '.'
    
    return os.path.join(input_dir, f"{input_name}_report.{extension}")


def save_report(content: bytes, output_path: str):
    """
    ä¿å­˜æŠ¥å‘Šæ–‡ä»¶
    
    Args:
        content: æŠ¥å‘Šå†…å®¹
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(output_path)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜æ–‡ä»¶
    with open(output_path, 'wb') as f:
        f.write(content)
    
    logger.info(f"âœ… æŠ¥å‘Šå·²ä¿å­˜è‡³: {os.path.abspath(output_path)}")

def main():
    """
    ä¸»å‡½æ•°
    """
    try:
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        args = parse_arguments()
        
        # å¦‚æœå¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œè®¾ç½®æ—¥å¿—çº§åˆ«ä¸ºDEBUG
        if args.debug:
            logging.getLogger().setLevel(logging.DEBUG)
        
        # åŠ è½½é…ç½®
        config = load_config(args.config)
        
        # æ„å»ºLLMé…ç½®
        llm_config = build_llm_config(config, args)
        
        # ç¡®å®šè¾“å‡ºè·¯å¾„
        output_path = determine_output_path(args.input, args.output, args.format)
        
        logger.info(f"ğŸš€ å¼€å§‹ç”Ÿæˆ{args.format.upper()}æ ¼å¼çš„Alphaå› å­åˆ†ææŠ¥å‘Š")
        logger.info(f"ğŸ¤– ä½¿ç”¨LLMæä¾›å•†: {llm_config.get('provider', 'openai')}, æ¨¡å‹: {llm_config.get('model_name', 'gpt-4')}")
        logger.info(f"ğŸ“¥ è¾“å…¥æ–‡ä»¶: {os.path.abspath(args.input)}")
        logger.info(f"ğŸ“¤ è¾“å‡ºæ–‡ä»¶: {os.path.abspath(output_path)}")
        
        # åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨
        report_generator = ReportGenerator(llm_config)
        
        # åŠ è½½è¾“å…¥æ•°æ®
        input_data = report_generator.load_input_data(args.input)
        
        # ç”ŸæˆæŠ¥å‘Š
        report_content = report_generator.generate_report(input_data, args.format)
        
        # ä¿å­˜æŠ¥å‘Š
        save_report(report_content, output_path)
        
        logger.info("ğŸ‰ æŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
        
    except KeyboardInterrupt:
        logger.info("âš ï¸ æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()