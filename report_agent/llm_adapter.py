#!/usr/bin/env python3
"""
LLMé€‚é…å™¨æ¨¡å—
æä¾›å¤šç§LLMæä¾›å•†çš„ç»Ÿä¸€æ¥å£ï¼Œæ”¯æŒé˜¿é‡Œç™¾ç‚¼ã€DeepSeekã€OpenAIç­‰
"""

import os
import time
import logging
from typing import Any, Dict, List, Optional
from abc import ABC, abstractmethod

from langchain_core.messages import HumanMessage, SystemMessage

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('report_agent.llm_adapter')


class BaseLLMAdapter(ABC):
    """
    LLMé€‚é…å™¨åŸºç±»
    å®šä¹‰æ‰€æœ‰LLMé€‚é…å™¨å¿…é¡»å®ç°çš„æ¥å£
    """
    
    def __init__(self, 
                 model_name: str,
                 temperature: float = 0.1,
                 max_tokens: int = 4000):
        """
        åˆå§‹åŒ–LLMé€‚é…å™¨
        
        Args:
            model_name: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
        """
        self.model_name = model_name
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.llm = self._init_llm()
        logger.info(f"âœ… LLMé€‚é…å™¨åˆå§‹åŒ–æˆåŠŸï¼Œæä¾›å•†: {self.__class__.__name__}ï¼Œæ¨¡å‹: {model_name}")
    
    @abstractmethod
    def _init_llm(self):
        """
        åˆå§‹åŒ–LLMå®ä¾‹
        å­ç±»å¿…é¡»å®ç°æ­¤æ–¹æ³•
        """
        pass
    
    def generate_report_section(self,
                               system_prompt: str,
                               user_prompt: str,
                               data: Optional[Dict] = None) -> str:
        """
        ç”ŸæˆæŠ¥å‘Šçš„ç‰¹å®šéƒ¨åˆ†
        
        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            data: é™„åŠ æ•°æ®ï¼Œå°†ä¸ç”¨æˆ·æç¤ºè¯åˆå¹¶
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
        """
        # æ„å»ºå®Œæ•´çš„ç”¨æˆ·æç¤ºè¯
        full_user_prompt = user_prompt
        if data:
            import json
            data_str = json.dumps(data, ensure_ascii=False, indent=2)
            full_user_prompt += f"\n\næ•°æ®:\n{data_str}"
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=full_user_prompt)
        ]
        
        # è®°å½•è¯·æ±‚ä¿¡æ¯
        logger.info(f"ğŸ“ ç”ŸæˆæŠ¥å‘Šéƒ¨åˆ†ï¼Œæ¨¡å‹: {self.model_name}")
        logger.debug(f"ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(system_prompt)}å­—ç¬¦")
        logger.debug(f"ç”¨æˆ·æç¤ºè¯é•¿åº¦: {len(full_user_prompt)}å­—ç¬¦")
        
        # è°ƒç”¨LLMç”Ÿæˆå†…å®¹
        start_time = time.time()
        try:
            response = self.llm.invoke(messages)
            content = response.content
            
            # è®°å½•å“åº”ä¿¡æ¯
            elapsed_time = time.time() - start_time
            logger.info(f"âœ… æŠ¥å‘Šéƒ¨åˆ†ç”ŸæˆæˆåŠŸï¼Œç”¨æ—¶: {elapsed_time:.2f}ç§’")
            logger.debug(f"ç”Ÿæˆå†…å®¹é•¿åº¦: {len(content)}å­—ç¬¦")
            
            return content
        except Exception as e:
            logger.error(f"âŒ æŠ¥å‘Šéƒ¨åˆ†ç”Ÿæˆå¤±è´¥: {str(e)}")
            raise Exception(f"ç”ŸæˆæŠ¥å‘Šéƒ¨åˆ†å¤±è´¥: {str(e)}")


class OpenAIAdapter(BaseLLMAdapter):
    """
    OpenAIé€‚é…å™¨
    æ”¯æŒOpenAIçš„GPTæ¨¡å‹
    """
    
    def __init__(self,
                 model_name: str = "gpt-4",
                 api_key: Optional[str] = None,
                 base_url: Optional[str] = None,
                 temperature: float = 0.1,
                 max_tokens: int = 4000):
        """
        åˆå§‹åŒ–OpenAIé€‚é…å™¨
        
        Args:
            model_name: æ¨¡å‹åç§°
            api_key: APIå¯†é’¥
            base_url: APIåŸºç¡€URL
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
        """
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.base_url = base_url
        
        if not self.api_key:
            raise ValueError("APIå¯†é’¥æœªæ‰¾åˆ°ï¼Œè¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡æˆ–ä¼ å…¥api_keyå‚æ•°")
        
        super().__init__(model_name, temperature, max_tokens)
    
    def _init_llm(self):
        """
        åˆå§‹åŒ–OpenAI LLMå®ä¾‹
        """
        from langchain_openai import ChatOpenAI
        
        llm_kwargs = {
            "model": self.model_name,
            "api_key": self.api_key,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens
        }
        
        # å¦‚æœæä¾›äº†è‡ªå®šä¹‰base_urlï¼Œåˆ™æ·»åŠ åˆ°å‚æ•°ä¸­
        if self.base_url:
            llm_kwargs["base_url"] = self.base_url
        
        return ChatOpenAI(**llm_kwargs)


class DashScopeAdapter(BaseLLMAdapter):
    """
    é˜¿é‡Œç™¾ç‚¼é€‚é…å™¨
    æ”¯æŒé˜¿é‡Œç™¾ç‚¼æ¨¡å‹
    """
    
    def __init__(self,
                 model_name: str = "qwen-plus",
                 api_key: Optional[str] = None,
                 temperature: float = 0.1,
                 max_tokens: int = 4000):
        """
        åˆå§‹åŒ–é˜¿é‡Œç™¾ç‚¼é€‚é…å™¨
        
        Args:
            model_name: æ¨¡å‹åç§°
            api_key: APIå¯†é’¥
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
        """
        self.api_key = api_key or os.getenv("DASHSCOPE_API_KEY")
        
        if not self.api_key:
            raise ValueError("APIå¯†é’¥æœªæ‰¾åˆ°ï¼Œè¯·è®¾ç½®DASHSCOPE_API_KEYç¯å¢ƒå˜é‡æˆ–ä¼ å…¥api_keyå‚æ•°")
        
        super().__init__(model_name, temperature, max_tokens)
    
    def _init_llm(self):
        """
        åˆå§‹åŒ–é˜¿é‡Œç™¾ç‚¼LLMå®ä¾‹
        """
        # å°è¯•å¯¼å…¥DashScopeç›¸å…³åº“
        try:
            from langchain_community.chat_models import ChatDashScope
            
            return ChatDashScope(
                model=self.model_name,
                dashscope_api_key=self.api_key,
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )
        except ImportError:
            # å¦‚æœæ²¡æœ‰å®‰è£…langchain_communityï¼Œå¯ä»¥ä½¿ç”¨OpenAIå…¼å®¹æ¨¡å¼
            logger.warning("langchain_communityæœªå®‰è£…ï¼Œå°è¯•ä½¿ç”¨OpenAIå…¼å®¹æ¨¡å¼è¿æ¥é˜¿é‡Œç™¾ç‚¼")
            from langchain_openai import ChatOpenAI
            
            return ChatOpenAI(
                model=self.model_name,
                api_key=self.api_key,
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )


class DeepSeekAdapter(BaseLLMAdapter):
    """
    DeepSeeké€‚é…å™¨
    æ”¯æŒDeepSeekæ¨¡å‹
    """
    
    def __init__(self,
                 model_name: str = "deepseek-chat",
                 api_key: Optional[str] = None,
                 temperature: float = 0.1,
                 max_tokens: int = 4000):
        """
        åˆå§‹åŒ–DeepSeeké€‚é…å™¨
        
        Args:
            model_name: æ¨¡å‹åç§°
            api_key: APIå¯†é’¥
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
        """
        self.api_key = api_key or os.getenv("DEEPSEEK_API_KEY")
        
        if not self.api_key:
            raise ValueError("APIå¯†é’¥æœªæ‰¾åˆ°ï¼Œè¯·è®¾ç½®DEEPSEEK_API_KEYç¯å¢ƒå˜é‡æˆ–ä¼ å…¥api_keyå‚æ•°")
        
        super().__init__(model_name, temperature, max_tokens)
    
    def _init_llm(self):
        """
        åˆå§‹åŒ–DeepSeek LLMå®ä¾‹
        """
        from langchain_openai import ChatOpenAI
        
        # DeepSeekä½¿ç”¨OpenAIå…¼å®¹æ¥å£
        return ChatOpenAI(
            model=self.model_name,
            api_key=self.api_key,
            base_url="https://api.deepseek.com/v1",
            temperature=self.temperature,
            max_tokens=self.max_tokens
        )


# LLMæä¾›å•†æ˜ å°„
LLM_PROVIDERS = {
    "openai": OpenAIAdapter,
    "dashscope": DashScopeAdapter,
    "deepseek": DeepSeekAdapter
}


def create_llm_adapter(config: Optional[Dict] = None) -> BaseLLMAdapter:
    """
    åˆ›å»ºLLMé€‚é…å™¨å®ä¾‹
    
    Args:
        config: LLMé…ç½®å­—å…¸ï¼Œå¿…é¡»åŒ…å«providerå­—æ®µ
        
    Returns:
        LLMé€‚é…å™¨å®ä¾‹
    """
    if config is None:
        config = {}
    
    # è·å–æä¾›å•†ç±»å‹ï¼Œé»˜è®¤ä¸ºopenai
    provider = config.get("provider", "openai").lower()
    
    # æ£€æŸ¥æä¾›å•†æ˜¯å¦æ”¯æŒ
    if provider not in LLM_PROVIDERS:
        supported = ", ".join(LLM_PROVIDERS.keys())
        raise ValueError(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {provider}ï¼Œæ”¯æŒçš„æä¾›å•†æœ‰: {supported}")
    
    # åˆ›å»ºå¯¹åº”çš„é€‚é…å™¨å®ä¾‹
    adapter_class = LLM_PROVIDERS[provider]
    
    # ç§»é™¤providerå­—æ®µï¼Œé¿å…ä¼ é€’ç»™é€‚é…å™¨æ„é€ å‡½æ•°
    adapter_config = config.copy()
    adapter_config.pop("provider", None)
    
    # æ ¹æ®ä¸åŒæä¾›å•†è®¾ç½®é»˜è®¤æ¨¡å‹
    if "model_name" not in adapter_config:
        if provider == "openai":
            adapter_config["model_name"] = "gpt-4"
        elif provider == "dashscope":
            adapter_config["model_name"] = "qwen-plus"
        elif provider == "deepseek":
            adapter_config["model_name"] = "deepseek-chat"
    
    return adapter_class(**adapter_config)


# ä¿ç•™æ—§çš„LLMAdapteråç§°ä»¥ä¿æŒå‘åå…¼å®¹æ€§
LLMAdapter = OpenAIAdapter