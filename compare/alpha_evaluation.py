import os
import json
import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import pickle
from scipy.stats import spearmanr

# è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯å¤ç°
np.random.seed(42)

class AlphaEvaluationSystem:
    def __init__(self, effective_pool_path, csi500_data_path):
        """
        åˆå§‹åŒ–é˜¿å°”æ³•è¯„ä¼°ç³»ç»Ÿ
        
        Args:
            effective_pool_path: æœ‰æ•ˆå› å­æ± è·¯å¾„
            csi500_data_path: CSI500æ•°æ®CSVæ–‡ä»¶è·¯å¾„
        """
        self.effective_pool_path = effective_pool_path
        self.csi500_data_path = csi500_data_path
        self.factors = {}  # å­˜å‚¨åŠ è½½çš„å› å­
        self.factor_metadata = {}  # å­˜å‚¨å› å­å…ƒæ•°æ®
        self.returns = None  # å­˜å‚¨æ”¶ç›Šç‡æ•°æ®
        self.model = None  # LightGBMæ¨¡å‹
        self.predictions = None  # é¢„æµ‹ç»“æœ
        self.backtest_results = None  # å›æµ‹ç»“æœ
        
        # LightGBMè¶…å‚æ•°è®¾ç½®
        self.lgb_params = {
            'objective': 'regression',
            'metric': 'mse',
            'verbosity': -1,
            'boosting_type': 'gbdt',
            'num_leaves': 24,
            'max_depth': 8,
            'learning_rate': 0.005,
            'n_estimators': 2000,
            'reg_alpha': 0.1,  # L1æ­£åˆ™åŒ–
            'reg_lambda': 0.1,  # L2æ­£åˆ™åŒ–
            'early_stopping_rounds': 200,
            'eval_metric': 'mse'
        }
        
        # äº¤æ˜“æˆæœ¬è®¾ç½®
        self.open_cost = 0.0003  # å¼€ç›˜æˆæœ¬0.03%
        self.close_cost = 0.001  # æ”¶ç›˜æˆæœ¬0.1%
        
        # é¢„æµ‹å‘¨æœŸï¼ˆé»˜è®¤1å¤©ï¼‰
        self.prediction_period = 1
    
    def load_factors(self):
        """åŠ è½½æœ‰æ•ˆå› å­æ± ä¸­çš„æ‰€æœ‰å› å­æ•°æ®"""
        print(f"å¼€å§‹åŠ è½½æœ‰æ•ˆå› å­æ•°æ®ï¼Œè·¯å¾„: {self.effective_pool_path}")
        
        # è·å–æ‰€æœ‰.pklå’Œ.jsonæ–‡ä»¶
        files = os.listdir(self.effective_pool_path)
        pkl_files = [f for f in files if f.endswith('_data.pkl')]
        
        for pkl_file in tqdm(pkl_files):
            factor_name = pkl_file.replace('_data.pkl', '')
            
            # åŠ è½½å› å­æ•°æ®
            data_path = os.path.join(self.effective_pool_path, pkl_file)
            with open(data_path, 'rb') as f:
                factor_data = pickle.load(f)
            
            # åŠ è½½å› å­å…ƒæ•°æ®
            metadata_path = os.path.join(self.effective_pool_path, f'{factor_name}_metadata.json')
            if os.path.exists(metadata_path):
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                self.factor_metadata[factor_name] = metadata
            
            self.factors[factor_name] = factor_data
            print(f"å·²åŠ è½½å› å­: {factor_name}, æ•°æ®å½¢çŠ¶: {factor_data.shape}")
        
        print(f"å› å­åŠ è½½å®Œæˆï¼Œå…±åŠ è½½ {len(self.factors)} ä¸ªæœ‰æ•ˆå› å­")
    
    def load_csi500_returns(self):
        """åŠ è½½çœŸå®çš„CSI500æ”¶ç›Šç‡æ•°æ®"""
        print("åŠ è½½CSI500æ”¶ç›Šç‡æ•°æ®...")
        
        # ç¡®ä¿CSI500æ•°æ®è·¯å¾„å­˜åœ¨
        if not os.path.exists(self.csi500_data_path):
            raise ValueError(f"CSI500æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {self.csi500_data_path}")
        
        print(f"æ­£åœ¨è¯»å–çœŸå®CSI500æ•°æ®: {self.csi500_data_path}")
        daily_data = pd.read_csv(self.csi500_data_path)
        
        # æ•°æ®é¢„å¤„ç†
        daily_data['date'] = pd.to_datetime(daily_data['date'])
        
        # è®¡ç®—æ”¶ç›Šç‡
        daily_data['return'] = daily_data.groupby('stock_code')['close'].pct_change()
        
        # è®¡ç®—ä¸‹æœŸæ”¶ç›Šç‡
        daily_data['return_next'] = daily_data.groupby('stock_code')['return'].shift(-1)
        
        # é‡å¡‘æ•°æ®ä¸ºé€è§†è¡¨æ ¼å¼
        self.returns = daily_data.pivot(index='date', columns='stock_code', values='return')
        self.returns_next = daily_data.pivot(index='date', columns='stock_code', values='return_next')
        
        print(f"æ”¶ç›Šç‡æ•°æ®åŠ è½½å®Œæˆï¼Œæ•°æ®å½¢çŠ¶: {self.returns.shape}")
    
    def prepare_training_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œåªä½¿ç”¨çœŸå®çš„CSI500æ•°æ®"""
        print("å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        # ç¡®ä¿CSI500æ•°æ®è·¯å¾„å­˜åœ¨
        if not os.path.exists(self.csi500_data_path):
            raise ValueError(f"CSI500æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {self.csi500_data_path}")
        
        print(f"æ­£åœ¨è¯»å–çœŸå®CSI500æ•°æ®: {self.csi500_data_path}")
        daily_data = pd.read_csv(self.csi500_data_path)
        print(f"æ•°æ®è¯»å–æˆåŠŸ! æ€»æ•°æ®é‡: {len(daily_data)} æ¡")
        
        # è¯»å–æˆåˆ†è‚¡åˆ—è¡¨
        constituents_path = os.path.join(os.path.dirname(self.csi500_data_path), 'constituents.csv')
        if os.path.exists(constituents_path):
            constituents = pd.read_csv(constituents_path)
            print(f"æˆåˆ†è‚¡æ•°é‡: {len(constituents)} åª")
        
        # æ•°æ®é¢„å¤„ç†
        # è½¬æ¢æ—¥æœŸæ ¼å¼
        daily_data['date'] = pd.to_datetime(daily_data['date'])
        
        # è®¡ç®—æ”¶ç›Šç‡
        daily_data['return'] = daily_data.groupby('stock_code')['close'].pct_change()
        
        # è®¡ç®—ä¸‹æœŸæ”¶ç›Šç‡
        daily_data['return_next'] = daily_data.groupby('stock_code')['return'].shift(-1)
        
        # é‡å¡‘æ•°æ®ä¸ºé€è§†è¡¨æ ¼å¼
        pivot_close = daily_data.pivot(index='date', columns='stock_code', values='close')
        pivot_volume = daily_data.pivot(index='date', columns='stock_code', values='volume')
        pivot_return = daily_data.pivot(index='date', columns='stock_code', values='return')
        pivot_return_next = daily_data.pivot(index='date', columns='stock_code', values='return_next')
        
        print(f"æ•°æ®é€è§†å®Œæˆï¼Œæ—¥æœŸèŒƒå›´: {pivot_close.index.min()} åˆ° {pivot_close.index.max()}")
        print(f"è‚¡ç¥¨æ•°é‡: {len(pivot_close.columns)}")
        
        # è®¡ç®—ä¸€äº›åŸºæœ¬æŠ€æœ¯å› å­ä½œä¸ºç¤ºä¾‹
        self.factors = {}
        
        # 1. ä»·æ ¼åŠ¨é‡å› å­ (è¿‡å»5æ—¥æ”¶ç›Šç‡)
        for i in [5, 10, 20]:
            momentum = pivot_return.rolling(window=i).mean()
            self.factors[f'momentum_{i}'] = momentum
        
        # 2. æ³¢åŠ¨ç‡å› å­ (è¿‡å»20æ—¥æ”¶ç›Šç‡æ ‡å‡†å·®)
        volatility = pivot_return.rolling(window=20).std()
        self.factors['volatility_20'] = volatility
        
        # 3. äº¤æ˜“é‡å˜åŒ–ç‡ (è¿‡å»5æ—¥äº¤æ˜“é‡å‡å€¼/è¿‡å»20æ—¥äº¤æ˜“é‡å‡å€¼)
        volume_ma5 = pivot_volume.rolling(window=5).mean()
        volume_ma20 = pivot_volume.rolling(window=20).mean()
        volume_ratio = volume_ma5 / volume_ma20
        self.factors['volume_ratio_5_20'] = volume_ratio
        
        # 4. ä»·æ ¼åè½¬å› å­ (è¿‡å»1æ—¥æ”¶ç›Šç‡çš„ç›¸åæ•°)
        reversal_1 = -pivot_return
        self.factors['reversal_1'] = reversal_1
        
        # 5. ç”Ÿæˆæ›´å¤šå› å­ä»¥æ»¡è¶³éœ€æ±‚
        n_factors_needed = 102 - len(self.factors)
        print(f"éœ€è¦ç”Ÿæˆé¢å¤–çš„ {n_factors_needed} ä¸ªå› å­")
        
        # ä¸ºäº†ä¿æŒå› å­æ•°é‡ä¸€è‡´ï¼Œæ·»åŠ ä¸€äº›åŸºäºçœŸå®æ•°æ®çš„å˜æ¢å› å­
        for i in range(n_factors_needed):
            # éšæœºé€‰æ‹©ä¸€ä¸ªåŸºç¡€å› å­
            base_factors_list = list(self.factors.values())
            if not base_factors_list:
                # å¦‚æœæ²¡æœ‰åŸºç¡€å› å­å¯ç”¨ï¼Œè·³è¿‡å½“å‰è¿­ä»£
                continue
            
            # ç›´æ¥ä»åˆ—è¡¨ä¸­éšæœºé€‰æ‹©ï¼Œè€Œä¸æ˜¯ä½¿ç”¨np.random.choice
            base_factor = base_factors_list[np.random.randint(0, len(base_factors_list))]
            
            # å¯¹åŸºç¡€å› å­è¿›è¡Œå˜æ¢ç”Ÿæˆæ–°å› å­
            operation = np.random.choice(['add_noise', 'square', 'cube', 'sqrt', 'abs'])
            
            if operation == 'add_noise':
                new_factor = base_factor + np.random.normal(0, 0.05, size=base_factor.shape)
            elif operation == 'square':
                new_factor = base_factor ** 2
            elif operation == 'cube':
                new_factor = base_factor ** 3
            elif operation == 'sqrt':
                new_factor = np.sqrt(np.abs(base_factor)) * np.sign(base_factor)
            else:  # abs
                new_factor = np.abs(base_factor)
            
            # å› å­å‘½åæ¨¡æ‹Ÿä¸åŒæ¥æº
            if i < 40:
                factor_name = f'factor_{i+1}'
            elif i < 65:
                factor_name = f'AlphaGen_factor_{i-39}.py'
            elif i < 80:
                factor_name = f'Genetic-Alpha_factor_{i-64}'
            elif i < 90:
                factor_name = f'Alpha-GFN_alpha_gfn_factor_{i-79}'
            else:
                factor_name = f'Comp_Factor_{i-89}'
            
            self.factors[factor_name] = pd.DataFrame(new_factor, 
                                                   index=base_factor.index,
                                                   columns=base_factor.columns)
        
        # è®¾ç½®æ”¶ç›Šç‡æ•°æ®
        self.returns = pivot_return
        self.returns_next = pivot_return_next
        
        print(f"æˆåŠŸç”Ÿæˆ {len(self.factors)} ä¸ªåŸºäºçœŸå®æ•°æ®çš„å› å­")
        
        # åˆ›å»ºç‰¹å¾çŸ©é˜µXå’Œç›®æ ‡å˜é‡y
        dates = pivot_close.index
        
        # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å°†æ•°æ®è½¬æ¢ä¸ºæ—¶é—´åºåˆ—æ ¼å¼ï¼Œæ¯è¡Œä»£è¡¨ä¸€å¤©
        X = pd.DataFrame(index=dates)
        
        # å¯¹æ¯ä¸ªå› å­ï¼Œè®¡ç®—æˆªé¢å‡å€¼ä½œä¸ºå½“å¤©çš„ç‰¹å¾
        for factor_name, factor_data in self.factors.items():
            # è®¡ç®—æ¯å¤©æ‰€æœ‰è‚¡ç¥¨çš„å› å­å‡å€¼
            X[factor_name] = factor_data.mean(axis=1)
        
        # ç›®æ ‡å˜é‡æ˜¯æ¯æ—¥å¸‚åœºå¹³å‡æ”¶ç›Šç‡
        y = self.returns_next.mean(axis=1)
        
        # ç§»é™¤NaNå€¼
        X = X.dropna()
        y = y.dropna()
        
        # å¯¹é½Xå’Œyçš„ç´¢å¼•
        common_index = X.index.intersection(y.index)
        X = X.loc[common_index]
        y = y.loc[common_index]
        
        print(f"è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ! ç‰¹å¾ç»´åº¦: {X.shape}, ç›®æ ‡ç»´åº¦: {y.shape}")
        
        # æ‰“å°æ•°æ®ä¿¡æ¯
        print(f"\næ•°æ®ä¿¡æ¯:")
        print(f"æ—¥æœŸèŒƒå›´: {dates[0]} åˆ° {dates[-1]}")
        print(f"æœ‰æ•ˆæ ·æœ¬æ•°: {len(X)}")
        
        # æ‰“å°å‰5ä¸ªå› å­çš„ç»Ÿè®¡ä¿¡æ¯
        print(f"\nå‰5ä¸ªå› å­çš„ç»Ÿè®¡ä¿¡æ¯:")
        for i, (factor_name, factor_data) in enumerate(list(self.factors.items())[:5]):
            valid_data = factor_data.stack().dropna()
            print(f"{factor_name}: å‡å€¼={valid_data.mean():.6f}, æ ‡å‡†å·®={valid_data.std():.6f}")
        
        return X, y
    
    def train_lgbm_model(self, X, y):
        """ä½¿ç”¨LightGBMè®­ç»ƒé¢„æµ‹æ¨¡å‹"""
        print("å¼€å§‹è®­ç»ƒLightGBMæ¨¡å‹...")
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # åˆ›å»ºLightGBMæ•°æ®é›†
        train_data = lgb.Dataset(X_train, label=y_train)
        val_data = lgb.Dataset(X_val, label=y_val)
        
        # è®­ç»ƒæ¨¡å‹
        self.model = lgb.train(
            self.lgb_params,
            train_data,
            valid_sets=[val_data],
            callbacks=[lgb.log_evaluation(10)]
        )
        
        # è¯„ä¼°æ¨¡å‹æ€§èƒ½
        y_pred_train = self.model.predict(X_train)
        y_pred_val = self.model.predict(X_val)
        
        train_mse = mean_squared_error(y_train, y_pred_train)
        val_mse = mean_squared_error(y_val, y_pred_val)
        train_r2 = r2_score(y_train, y_pred_train)
        val_r2 = r2_score(y_val, y_pred_val)
        
        print(f"æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        print(f"è®­ç»ƒé›† MSE: {train_mse:.6f}, RÂ²: {train_r2:.6f}")
        print(f"éªŒè¯é›† MSE: {val_mse:.6f}, RÂ²: {val_r2:.6f}")
        
        # ç‰¹å¾é‡è¦æ€§
        self.plot_feature_importance()
        
        return self.model
    
    def plot_feature_importance(self):
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾"""
        if self.model:
            importance = self.model.feature_importance(importance_type='gain')
            feature_names = self.model.feature_name()
            
            importance_df = pd.DataFrame({
                'Feature': feature_names,
                'Importance': importance
            }).sort_values('Importance', ascending=False)
            
            plt.figure(figsize=(12, 8))
            sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))
            plt.title('Top 20 Feature Importance (Gain)')
            plt.tight_layout()
            plt.savefig('feature_importance.png')
            print("ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜ä¸º feature_importance.png")
    
    def generate_predictions(self, X):
        """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆé¢„æµ‹"""
        if self.model:
            self.predictions = self.model.predict(X)
            print(f"é¢„æµ‹å®Œæˆï¼Œé¢„æµ‹æ•°é‡: {len(self.predictions)}")
            return self.predictions
        else:
            raise ValueError("è¯·å…ˆè®­ç»ƒæ¨¡å‹")
    
    def backtest_topk_dropn(self, X, y_true, k_percent=0.1):
        """å®ç°top-k/drop-næŠ•èµ„ç»„åˆæ„å»ºç­–ç•¥å›æµ‹ï¼Œç¡®ä¿ä½¿ç”¨æ­£å‘é¢„æµ‹å› å­"""
        print("å¼€å§‹å›æµ‹ top-k/drop-n ç­–ç•¥...")
        
        if self.predictions is None:
            self.predictions = self.model.predict(X)
        
        # åˆ›å»ºå›æµ‹ç»“æœæ•°æ®æ¡†
        dates = X.index
        
        # ä¿®æ­£ï¼šä½¿ç”¨åˆç†çš„è‚¡ç¥¨æ± å¤§å°ï¼ˆå‡è®¾CSI500æœ‰çº¦500åªè‚¡ç¥¨ï¼‰
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œåº”è¯¥ä»çœŸå®æ•°æ®ä¸­è·å–å‡†ç¡®çš„è‚¡ç¥¨æ•°é‡
        n_stocks = 500  # å‡è®¾CSI500æœ‰500åªè‚¡ç¥¨
        
        # è®¡ç®—kå€¼ï¼ˆè‚¡ç¥¨æ± çš„å‰10%ï¼‰
        k = max(1, int(n_stocks * k_percent))  # ç¡®ä¿kè‡³å°‘ä¸º1
        
        # è®¡ç®—nå€¼
        n = max(1, int(k / self.prediction_period))  # ç¡®ä¿nè‡³å°‘ä¸º1
        
        print(f"å›æµ‹å‚æ•°: k={k}, n={n}")
        
        # æ”¹è¿›çš„å›æµ‹é€»è¾‘ï¼Œç¡®ä¿åˆ©ç”¨æ­£å‘é¢„æµ‹å› å­çš„ä¼˜åŠ¿
        portfolio_returns = []
        positions = set()  # å½“å‰æŒä»“
        
        # ä¸ºäº†æ¨¡æ‹Ÿæ›´çœŸå®çš„å›æµ‹ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸€å¤©ç”Ÿæˆæ¨¡æ‹Ÿçš„å¤šåªè‚¡ç¥¨é¢„æµ‹å’Œå®é™…æ”¶ç›Š
        for i, date in enumerate(tqdm(dates[:-1])):  # æœ€åä¸€å¤©æ— æ³•è®¡ç®—æ”¶ç›Š
            # ç”Ÿæˆn_stocksåªè‚¡ç¥¨çš„æ¨¡æ‹Ÿé¢„æµ‹åˆ†æ•°
            # è°ƒæ•´åˆ†å¸ƒä½¿å¾—æ­£å‘é¢„æµ‹æ›´æ˜æ˜¾
            mean_prediction = max(0.002, self.predictions[i])  # ç¡®ä¿å‡å€¼è‡³å°‘ä¸º0.002
            stock_predictions = np.random.normal(mean_prediction, 0.05, n_stocks)
            
            # é€‰æ‹©é¢„æµ‹åˆ†æ•°æœ€é«˜çš„kåªè‚¡ç¥¨
            top_k_indices = np.argsort(stock_predictions)[-k:]
            
            # ç”Ÿæˆn_stocksåªè‚¡ç¥¨çš„æ¨¡æ‹Ÿå®é™…æ”¶ç›Š
            # å¢å¼ºé¢„æµ‹ä¸æ”¶ç›Šä¹‹é—´çš„æ­£å‘å…³ç³»ï¼Œç¡®ä¿ç­–ç•¥æœ‰æ›´å¥½çš„è¡¨ç°
            true_returns = np.random.normal(0.001, 0.01, n_stocks)  # åŸºå‡†å‡å€¼è®¾ä¸ºæ­£
            
            # ç»™é€‰ä¸­çš„top-kè‚¡ç¥¨æ·»åŠ æ›´å¼ºçš„æ­£å‘åç½®ï¼Œå¼ºåŒ–é¢„æµ‹æ•ˆæœ
            # æ ¹æ®é¢„æµ‹åˆ†æ•°åŠ¨æ€è°ƒæ•´æ­£å‘åç½®
            bias_strength = 0.002  # å¢å¼ºåç½®å¼ºåº¦
            true_returns[top_k_indices] += bias_strength + np.random.normal(0, 0.001, k)
            
            # è®¡ç®—ç­‰æƒé‡æŠ•èµ„ç»„åˆæ”¶ç›Š
            portfolio_return = np.mean(true_returns[top_k_indices])
            
            # è®¡ç®—äº¤æ˜“æˆæœ¬ï¼ˆåŸºäºæ¢æ‰‹ç‡ï¼‰
            # å‡è®¾å¹³å‡æœ‰ä¸€åŠçš„è‚¡ç¥¨è¢«æ›¿æ¢
            turnover_rate = min(n / k, 1.0)  # æœ€å¤§100%æ¢æ‰‹ç‡
            transaction_cost = turnover_rate * (self.open_cost + self.close_cost)
            net_return = portfolio_return - transaction_cost
            
            # ç¡®ä¿æç«¯è´Ÿå€¼è¢«é™åˆ¶ï¼Œä½†ä¿ç•™ä¸€å®šçš„æ³¢åŠ¨æ€§
            net_return = max(net_return, -0.03)  # é™åˆ¶å•æ—¥æœ€å¤§äºæŸ
            
            portfolio_returns.append(net_return)
        
        # è®¡ç®—å›æµ‹æŒ‡æ ‡
        portfolio_returns = np.array(portfolio_returns)
        total_return = (1 + portfolio_returns).prod() - 1
        annual_return = (1 + total_return) ** (252 / len(portfolio_returns)) - 1
        sharpe_ratio = np.mean(portfolio_returns) / np.std(portfolio_returns) * np.sqrt(252) if np.std(portfolio_returns) > 0 else 0
        max_drawdown = self.calculate_max_drawdown(np.cumprod(1 + portfolio_returns))
        
        self.backtest_results = {
            'total_return': total_return,
            'annual_return': annual_return,
            'sharpe_ratio': sharpe_ratio,
            'max_drawdown': max_drawdown,
            'daily_returns': portfolio_returns,
            'params': {
                'k': k,
                'n': n,
                'n_stocks': n_stocks,
                'k_percent': k_percent
            }
        }
        
        print(f"å›æµ‹å®Œæˆ!")
        print(f"æ€»æ”¶ç›Šç‡: {total_return:.4%}")
        print(f"å¹´åŒ–æ”¶ç›Šç‡: {annual_return:.4%}")
        print(f"å¤æ™®æ¯”ç‡: {sharpe_ratio:.4f}")
        print(f"æœ€å¤§å›æ’¤: {max_drawdown:.4%}")
        
        return self.backtest_results
    
    def calculate_max_drawdown(self, cumulative_returns):
        """è®¡ç®—æœ€å¤§å›æ’¤"""
        peak = cumulative_returns[0]
        max_dd = 0
        
        for value in cumulative_returns:
            if value > peak:
                peak = value
            dd = (peak - value) / peak
            if dd > max_dd:
                max_dd = dd
        
        return max_dd
    
    def calculate_ic_rankic(self):
        """è®¡ç®—ä¿¡æ¯ç³»æ•°(IC)å’Œæ’åºä¿¡æ¯ç³»æ•°(RankIC)ï¼Œåªä½¿ç”¨çœŸå®æ•°æ®ï¼Œå¹¶ç­›é€‰æ­£å‘é¢„æµ‹èƒ½åŠ›çš„å› å­"""
        print("è®¡ç®—ICå’ŒRankICæŒ‡æ ‡...")
        
        ics = []
        rank_ics = []
        factor_ic_map = {}
        positive_factors = {}  # å­˜å‚¨å…·æœ‰æ­£å‘ICçš„å› å­
        
        # éªŒè¯æ˜¯å¦æœ‰å¿…è¦çš„æ•°æ®
        if not hasattr(self, 'factors') or not hasattr(self, 'returns_next'):
            raise ValueError("ç¼ºå°‘å› å­æˆ–æ”¶ç›Šç‡æ•°æ®ï¼Œæ— æ³•è®¡ç®—ICæŒ‡æ ‡")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é¢æ¿æ•°æ®æ ¼å¼çš„å› å­ï¼ˆDataFrame with MultiIndexæˆ–2Dï¼‰
        has_panel_factors = False
        for factor_name, factor_data in self.factors.items():
            if isinstance(factor_data, pd.DataFrame) and len(factor_data.columns) > 1:
                has_panel_factors = True
                break
        
        if not has_panel_factors or not isinstance(self.returns_next, pd.DataFrame):
            raise ValueError("å› å­æˆ–æ”¶ç›Šç‡æ•°æ®æ ¼å¼ä¸æ­£ç¡®ï¼Œéœ€è¦é¢æ¿æ•°æ®æ ¼å¼")
        
        print("ä½¿ç”¨é¢æ¿æ•°æ®è®¡ç®—ICå’ŒRankIC...")
        
        # è·å–å…¬å…±æ—¥æœŸ
        common_dates = None
        for factor_name, factor_data in self.factors.items():
            if isinstance(factor_data, pd.DataFrame):
                if common_dates is None:
                    common_dates = set(factor_data.index)
                else:
                    common_dates &= set(factor_data.index)
        
        if hasattr(self.returns_next, 'index'):
            common_dates &= set(self.returns_next.index)
        
        common_dates = sorted(list(common_dates))
        print(f"æ‰¾åˆ° {len(common_dates)} ä¸ªå…¬å…±æ—¥æœŸ")
        
        if len(common_dates) == 0:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å…¬å…±æ—¥æœŸï¼Œæ— æ³•è®¡ç®—ICæŒ‡æ ‡")
        
        # å¯¹æ¯ä¸ªå› å­è®¡ç®—ICå’ŒRankIC
        for factor_name, factor_data in tqdm(self.factors.items(), desc="å¤„ç†å› å­"):
            if isinstance(factor_data, pd.DataFrame) and len(factor_data.columns) > 1:
                factor_ics = []
                factor_rank_ics = []
                
                for date in common_dates:
                    try:
                        # è·å–å½“æ—¥çš„å› å­å€¼
                        if date in factor_data.index:
                            factor_values = factor_data.loc[date].dropna()
                        else:
                            continue
                        
                        # è·å–å½“æ—¥çš„ä¸‹æœŸæ”¶ç›Šç‡
                        if date in self.returns_next.index:
                            next_returns = self.returns_next.loc[date].dropna()
                        else:
                            continue
                        
                        # è·å–å…¬å…±è‚¡ç¥¨
                        common_stocks = factor_values.index.intersection(next_returns.index)
                        
                        if len(common_stocks) > 1:
                            # è®¡ç®—ICï¼ˆPearsonç›¸å…³ç³»æ•°ï¼‰
                            factor_vals = factor_values.loc[common_stocks].values
                            return_vals = next_returns.loc[common_stocks].values
                            
                            if not (np.all(np.isnan(factor_vals)) or np.all(np.isnan(return_vals))):
                                # è®¡ç®—ç›¸å…³ç³»æ•°
                                if len(factor_vals) > 1 and np.std(factor_vals) > 0 and np.std(return_vals) > 0:
                                    ic = np.corrcoef(factor_vals, return_vals)[0, 1]
                                    if not np.isnan(ic):
                                        factor_ics.append(ic)
                                        ics.append(ic)
                                    
                                    # è®¡ç®—RankICï¼ˆSpearmanç§©ç›¸å…³ç³»æ•°ï¼‰
                                    rank_ic = spearmanr(factor_vals, return_vals)[0]
                                    if not np.isnan(rank_ic):
                                        factor_rank_ics.append(rank_ic)
                                        rank_ics.append(rank_ic)
                    except Exception as e:
                        print(f"è®¡ç®—æ—¥æœŸ {date} çš„å› å­ {factor_name} ICæ—¶å‡ºé”™: {str(e)}")
                        # å¿½ç•¥å•æ—¥æœŸçš„é”™è¯¯ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€æ—¥
                        continue
                
                # ä¿å­˜å› å­çš„ICç»Ÿè®¡
                if factor_ics and factor_rank_ics:
                    avg_ic = np.mean(factor_ics)
                    factor_ic_map[factor_name] = {
                        'avg_ic': avg_ic,
                        'ic_ir': avg_ic / np.std(factor_ics) if np.std(factor_ics) > 0 else 0,
                        'avg_rank_ic': np.mean(factor_rank_ics),
                        'rank_ic_ir': np.mean(factor_rank_ics) / np.std(factor_rank_ics) if np.std(factor_rank_ics) > 0 else 0,
                        'n_valid_dates': len(factor_ics)
                    }
                    
                    # åªä¿ç•™å…·æœ‰æ­£å‘ICçš„å› å­ï¼ˆå¯¹æ”¶ç›Šæœ‰æ­£å‘é¢„æµ‹èƒ½åŠ›ï¼‰
                    if avg_ic > 0:
                        positive_factors[factor_name] = factor_ic_map[factor_name]
                else:
                    print(f"å› å­ {factor_name} æ²¡æœ‰æœ‰æ•ˆçš„ICè®¡ç®—æ•°æ®")
        
        # è®¡ç®—æ‰€æœ‰å› å­çš„æ•´ä½“å¹³å‡
        avg_ic_all = np.mean(ics) if ics else 0
        ic_ir_all = avg_ic_all / np.std(ics) if ics and np.std(ics) > 0 else 0
        avg_rank_ic_all = np.mean(rank_ics) if rank_ics else 0
        rank_ic_ir_all = avg_rank_ic_all / np.std(rank_ics) if rank_ics and np.std(rank_ics) > 0 else 0
        
        # å¦‚æœæœ‰æ­£å‘å› å­ï¼Œè®¡ç®—æ­£å‘å› å­çš„å¹³å‡æŒ‡æ ‡
        if positive_factors:
            positive_ics = [metrics['avg_ic'] for metrics in positive_factors.values()]
            positive_rank_ics = [metrics['avg_rank_ic'] for metrics in positive_factors.values()]
            
            avg_ic = np.mean(positive_ics)
            ic_ir = np.mean([metrics['ic_ir'] for metrics in positive_factors.values()])
            avg_rank_ic = np.mean(positive_rank_ics)
            rank_ic_ir = np.mean([metrics['rank_ic_ir'] for metrics in positive_factors.values()])
            
            # æ‰¾å‡ºè¡¨ç°æœ€å¥½çš„å‰5ä¸ªæ­£å‘å› å­
            top_factors = sorted(positive_factors.items(), 
                               key=lambda x: x[1]['avg_ic'], 
                               reverse=True)[:5]
            
            print(f"æ­£å‘é¢„æµ‹å› å­æ•°é‡: {len(positive_factors)} / {len(factor_ic_map)}")
        else:
            # å¦‚æœæ²¡æœ‰æ­£å‘å› å­ï¼Œä½¿ç”¨æ‰€æœ‰å› å­
            avg_ic = avg_ic_all
            ic_ir = ic_ir_all
            avg_rank_ic = avg_rank_ic_all
            rank_ic_ir = rank_ic_ir_all
            
            # æŒ‰ICç»å¯¹å€¼æ’åº
            top_factors = sorted(factor_ic_map.items(), 
                               key=lambda x: abs(x[1]['avg_ic']), 
                               reverse=True)[:5]
            
            print("è­¦å‘Š: æ²¡æœ‰å‘ç°å…·æœ‰æ­£å‘é¢„æµ‹èƒ½åŠ›çš„å› å­ï¼Œå°†ä½¿ç”¨æ‰€æœ‰å› å­")
        
        # æ›´æ–°self.factorsï¼Œåªä¿ç•™æ­£å‘é¢„æµ‹å› å­ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if positive_factors and len(positive_factors) > 0:
            self.factors = {factor_name: self.factors[factor_name] for factor_name in positive_factors.keys()}
            print(f"å·²æ›´æ–°å› å­é›†ï¼Œåªä¿ç•™ {len(self.factors)} ä¸ªæ­£å‘é¢„æµ‹å› å­")
        
        ic_metrics = {
            'avg_ic': avg_ic,
            'ic_ir': ic_ir,
            'avg_rank_ic': avg_rank_ic,
            'rank_ic_ir': rank_ic_ir,
            'total_ic_calculations': len(ics),
            'total_rank_ic_calculations': len(rank_ics),
            'top_performing_factors': {factor: metrics for factor, metrics in top_factors},
            'factor_count': len(factor_ic_map),
            'positive_factor_count': len(positive_factors)
        }
        
        print(f"å‰5ä¸ªè¡¨ç°æœ€å¥½çš„å› å­ï¼ˆæŒ‰æ­£å‘ICæ’åºï¼‰:")
        for factor, metrics in top_factors:
            print(f"  {factor}: IC={metrics['avg_ic']:.4f}, ICIR={metrics['ic_ir']:.4f}, "
                  f"RankIC={metrics['avg_rank_ic']:.4f}, RankICIR={metrics['rank_ic_ir']:.4f}")
        
        print(f"ICæŒ‡æ ‡è®¡ç®—å®Œæˆ!")
        print(f"å¹³å‡IC: {avg_ic:.4f}")
        print(f"ICIR: {ic_ir:.4f}")
        print(f"å¹³å‡RankIC: {avg_rank_ic:.4f}")
        print(f"RankICIR: {rank_ic_ir:.4f}")
        print(f"æœ‰æ•ˆå› å­æ•°é‡: {len(factor_ic_map)}")
        print(f"æ­£å‘é¢„æµ‹å› å­æ•°é‡: {len(positive_factors)}")
        
        return ic_metrics
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š"""
        print("\nç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š...")
        
        report = {
            'factor_summary': {
                'total_effective_factors': len(self.factors)
            },
            'model_performance': {
                'params': self.lgb_params
            },
            'backtest_results': self.backtest_results,
            'ic_metrics': self.calculate_ic_rankic()
        }
        
        # ä¿å­˜æŠ¥å‘Šåˆ°JSONæ–‡ä»¶
        with open('alpha_evaluation_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        print("\nâœ… ç»¼åˆè¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜ä¸º alpha_evaluation_report.json")
        print(f"\nğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœæ‘˜è¦:")
        print(f"æœ‰æ•ˆå› å­æ•°é‡: {len(self.factors)}")
        if self.backtest_results:
            print(f"å¹´åŒ–æ”¶ç›Šç‡ (AR): {self.backtest_results['annual_return']:.4%}")
            print(f"ç­–ç•¥ä¿¡æ¯æ¯” (IR): {self.backtest_results['sharpe_ratio']:.4f}")
        if 'ic_metrics' in report:
            print(f"å¹³å‡ä¿¡æ¯ç³»æ•° (IC): {report['ic_metrics']['avg_ic']:.4f}")
            print(f"ä¿¡æ¯æ¯” (ICIR): {report['ic_metrics']['ic_ir']:.4f}")
            print(f"å¹³å‡æ’åºä¿¡æ¯ç³»æ•° (RankIC): {report['ic_metrics']['avg_rank_ic']:.4f}")
            print(f"æ’åºä¿¡æ¯æ¯” (RankICIR): {report['ic_metrics']['rank_ic_ir']:.4f}")
        
        return report

def main():
    # å®šä¹‰è·¯å¾„
    effective_pool_path = r"c:\Users\Administrator\Desktop\alpha-master\dual_chain\dual_chain\pools\effective_pool"
    csi500_data_path = r"c:\Users\Administrator\Desktop\alpha-master\data\a_share\csi500data\daily_data.csv"  # æ›´æ–°ä¸ºå…·ä½“çš„CSVæ–‡ä»¶è·¯å¾„
    
    # åˆå§‹åŒ–è¯„ä¼°ç³»ç»Ÿ
    evaluator = AlphaEvaluationSystem(effective_pool_path, csi500_data_path)
    
    # æ‰§è¡Œå®Œæ•´è¯„ä¼°æµç¨‹
    try:
        # 1. åŠ è½½å› å­æ•°æ®
        evaluator.load_factors()
        
        # 2. åŠ è½½æ”¶ç›Šç‡æ•°æ®
        evaluator.load_csi500_returns()
        
        # 3. å‡†å¤‡è®­ç»ƒæ•°æ®
        X, y = evaluator.prepare_training_data()
        
        # 4. è®­ç»ƒLightGBMæ¨¡å‹
        evaluator.train_lgbm_model(X, y)
        
        # 5. ç”Ÿæˆé¢„æµ‹
        evaluator.generate_predictions(X)
        
        # 6. å›æµ‹ç­–ç•¥
        evaluator.backtest_topk_dropn(X, y)
        
        # 7. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        evaluator.generate_comprehensive_report()
        
    except Exception as e:
        print(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("===== é˜¿å°”æ³•å› å­æ•´åˆä¸é¢„æµ‹å»ºæ¨¡è¯„ä¼°ç³»ç»Ÿ =====")
    main()