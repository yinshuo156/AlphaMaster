# -*- coding: utf-8 -*-
"""
LLMå› å­é€‚é…å™¨
ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå’Œä¼˜åŒ–å› å­è¡¨è¾¾å¼
"""

import os
import json
import time
import logging
from typing import Dict, List, Tuple, Optional, Any
import random
import re

from report_agent.llm_adapter import BaseLLMAdapter

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('dual_chain.llm_factor_adapter')

class LLMToolError(Exception):
    """LLMå·¥å…·é”™è¯¯"""
    pass

class LLMFactorAdapter(BaseLLMAdapter):
    """
    LLMå› å­é€‚é…å™¨
    ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå’Œä¼˜åŒ–å› å­
    """
    
    def __init__(self, model_name: str = "gpt-4", temperature: float = 0.3, mock_mode: bool = False, provider: str = None, api_key: str = None):
        """
        åˆå§‹åŒ–LLMå› å­é€‚é…å™¨
        
        Args:
            model_name: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            mock_mode: æ˜¯å¦å¯ç”¨æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆç”¨äºæµ‹è¯•ï¼Œä¸è°ƒç”¨çœŸå®APIï¼‰
            provider: LLMæä¾›å•†ï¼Œæ”¯æŒ'openai'ã€'dashscope'(é˜¿é‡Œç™¾ç‚¼)ã€'deepseek'ç­‰
            api_key: APIå¯†é’¥ï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è·å–
        """
        self.mock_mode = mock_mode
        self.provider = provider
        self.api_key = api_key
        
        if not mock_mode:
            try:
                # æ ¹æ®æ¨¡å‹åç§°è‡ªåŠ¨è¯†åˆ«æä¾›å•†
                if provider is None:
                    if 'qwen' in model_name.lower() or 'dashscope' in model_name.lower():
                        self.provider = 'dashscope'
                    elif 'deepseek' in model_name.lower():
                        self.provider = 'deepseek'
                    else:
                        self.provider = 'openai'
                
                logger.info(f"ğŸ“¡ æ­£åœ¨åˆå§‹åŒ–{self.provider}æ¨¡å‹: {model_name}")
                
                # æ ¹æ®æä¾›å•†é€‰æ‹©å¯¹åº”çš„é€‚é…å™¨
                if self.provider == 'dashscope':
                    from report_agent.llm_adapter import DashScopeAdapter
                    self.llm_adapter = DashScopeAdapter(
                        model_name=model_name,
                        api_key=api_key,
                        temperature=temperature,
                        max_tokens=2000
                    )
                elif self.provider == 'deepseek':
                    from report_agent.llm_adapter import DeepSeekAdapter
                    self.llm_adapter = DeepSeekAdapter(
                        model_name=model_name,
                        api_key=api_key,
                        temperature=temperature,
                        max_tokens=2000
                    )
                else:
                    # é»˜è®¤ä½¿ç”¨OpenAI
                    from report_agent.llm_adapter import OpenAIAdapter
                    self.llm_adapter = OpenAIAdapter(
                        model_name=model_name,
                        api_key=api_key,
                        temperature=temperature,
                        max_tokens=2000
                    )
                    
            except Exception as e:
                logger.warning(f"âš ï¸  LLMåˆå§‹åŒ–å¤±è´¥ï¼Œåˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ¨¡å¼: {e}")
                self.mock_mode = True
        
        if self.mock_mode:
            logger.info("â„¹ï¸  LLMFactorAdapter: å·²å¯ç”¨æ¨¡æ‹Ÿæ¨¡å¼ï¼Œä¸ä¼šè°ƒç”¨çœŸå®LLM API")
        
        self.prompt_templates = self._load_prompt_templates()
    
    def _load_prompt_templates(self) -> Dict[str, str]:
        """
        åŠ è½½æç¤ºè¯æ¨¡æ¿
        """
        templates = {
            "factor_generation": """
ç›®æ ‡
æ‚¨æ˜¯é‡åŒ–äº¤æ˜“é˜¿å°”æ³•å› å­ç”Ÿæˆé¢†åŸŸçš„ä¸“å®¶ã€‚æ‚¨çš„ä»»åŠ¡æ˜¯è®¾è®¡ä¸€ä¸ªæ–°çš„æ¨ªæˆªé¢é˜¿å°”æ³•å› å­â€”â€” è¿™æ˜¯ä¸€ç§æ•°å­¦è¡¨è¾¾å¼ï¼Œèƒ½å¤ŸåŸºäºè‚¡ç¥¨è¿‘æœŸçš„å¸‚åœºæ•°æ®ï¼Œåœ¨æ¯ä¸ªäº¤æ˜“æ—¥ä¸ºæ¯åªè‚¡ç¥¨åˆ†é…ä¸€ä¸ªåˆ†æ•°ã€‚è¯¥åˆ†æ•°å°†ç”¨äºå½“æ—¥å¯¹è‚¡ç¥¨è¿›è¡Œæ’åä¸ç­›é€‰ï¼ˆå³è·¨å¸‚åœºæ¨ªæˆªé¢æ“ä½œï¼‰ã€‚
æ‚¨å°†è·å¾—ï¼š
å¯ç”¨æ•°æ®å­—æ®µä¸è¿ç®—ç¬¦åˆ—è¡¨ã€‚
æœ‰æ•ˆä¸æ— æ•ˆå› å­çš„ç¤ºä¾‹é›†åˆã€‚
æ‚¨çš„ç›®æ ‡æ˜¯ç”Ÿæˆæ»¡è¶³ä»¥ä¸‹è¦æ±‚çš„å› å­è¡¨è¾¾å¼ï¼š
ä¸ç¤ºä¾‹å› å­ä¸åŒã€‚
ä»æœ‰æ•ˆå› å­ä¸­è·å–çµæ„Ÿã€‚
æ½œåœ¨é¢„æµ‹èƒ½åŠ›ä¼˜äºæ— æ•ˆå› å­ã€‚

å¯ç”¨æ•°æ®å­—æ®µ
æ‚¨å¯ä½¿ç”¨ä»¥ä¸‹æ•°æ®å­—æ®µæ„å»ºè¡¨è¾¾å¼ã€‚è¯·ç¡®ä¿åœ¨è¡¨è¾¾å¼ä¸­ç›´æ¥ä½¿ç”¨è¿™äº›å­—æ®µåï¼š
- close: æ”¶ç›˜ä»·
- open: å¼€ç›˜ä»·
- high: æœ€é«˜ä»·
- low: æœ€ä½ä»·
- volume: æˆäº¤é‡

å¯ç”¨è¿ç®—ç¬¦
æ‚¨å¯ä½¿ç”¨pandasçš„æ ‡å‡†æ“ä½œç¬¦å’Œå‡½æ•°ï¼Œå¦‚pct_changeã€rollingã€meanã€stdã€diffç­‰

å‚è€ƒå› å­
è¯·å‚è€ƒä»¥ä¸‹å› å­ï¼š
æœ‰æ•ˆå› å­ï¼š
{effective_factors}

æ— æ•ˆå› å­ï¼š
{discarded_factors}

è¦æ±‚
ä»æœ‰æ•ˆå› å­ä¸­è·å–çµæ„Ÿï¼Œé¿å…æ— æ•ˆå› å­ä¸­å‡ºç°çš„æ¨¡å¼ï¼ˆå¦‚ä½æ’åºä¿¡æ¯ç³»æ•°ï¼ˆRankICï¼‰æˆ–ä½ä¿¡æ¯æ¯”ï¼ˆIRï¼‰ï¼‰ã€‚
å› å­åº”ç”Ÿæˆæ— é‡çº²å€¼ï¼Œä¸å—ä»·æ ¼å°ºåº¦æˆ–æˆäº¤é‡å•ä½çš„å½±å“ã€‚
ä»…ä½¿ç”¨æä¾›çš„æ•°æ®å­—æ®µä¸è¿ç®—ç¬¦ã€‚
æ— éœ€ä½¿ç”¨æ‰€æœ‰æ•°æ®å­—æ®µï¼Œå› å­è¡¨è¾¾å¼ä¸­ä½¿ç”¨çš„æ•°æ®å­—æ®µæ€»æ•°ä¸è¶…è¿‡ 3 ä¸ªã€‚
é¿å…è¿‡åº¦åµŒå¥—è¿ç®—ç¬¦æˆ–æ„å»ºè¿‡äºå¤æ‚çš„è¡¨è¾¾å¼ï¼Œä»¥é™ä½è¿‡æ‹Ÿåˆé£é™©ã€‚
è¡¨è¾¾å¼åº”ç®€æ´ã€æ˜“è¯»ä¸”å¯è§£é‡Šã€‚

è¾“å‡ºæ ¼å¼
è¯·è¿”å›ä»¥ä¸‹æ ¼å¼ï¼š
è¡¨è¾¾å¼: your_factor_expression
è§£é‡Š: your_factor_explanation
            """,
            
            "factor_optimization": """
ç›®æ ‡
æ‚¨æ˜¯é‡åŒ–äº¤æ˜“é˜¿å°”æ³•å› å­ä¼˜åŒ–é¢†åŸŸçš„ä¸“å®¶ã€‚æ‚¨çš„ä»»åŠ¡æ˜¯ä¼˜åŒ–ä¸€ä¸ªç°æœ‰çš„æ¨ªæˆªé¢é˜¿å°”æ³•å› å­â€”â€” è¿™æ˜¯ä¸€ç§æ•°å­¦è¡¨è¾¾å¼ï¼Œèƒ½å¤ŸåŸºäºè‚¡ç¥¨è¿‘æœŸçš„å¸‚åœºæ•°æ®ï¼Œåœ¨æ¯ä¸ªäº¤æ˜“æ—¥ä¸ºæ¯åªè‚¡ç¥¨åˆ†é…ä¸€ä¸ªåˆ†æ•°ã€‚è¯¥åˆ†æ•°å°†ç”¨äºå½“æ—¥å¯¹è‚¡ç¥¨è¿›è¡Œæ’åä¸ç­›é€‰ï¼ˆå³è·¨å¸‚åœºæ¨ªæˆªé¢æ“ä½œï¼‰ã€‚
æ‚¨å°†è·å¾—ï¼š
å¯ç”¨æ•°æ®å­—æ®µä¸è¿ç®—ç¬¦åˆ—è¡¨ã€‚
ç°æœ‰å› å­çš„è¡¨è¾¾å¼åŠå…¶æ€§èƒ½ï¼ˆå¦‚æ’åºä¿¡æ¯ç³»æ•°ï¼ˆRankICï¼‰ã€æ’åºä¿¡æ¯æ¯”ï¼ˆRankIRï¼‰ã€æ¢æ‰‹ç‡ï¼ˆTurnoverï¼‰ã€å¤šæ ·æ€§ï¼ˆDiversityï¼‰ï¼‰ã€‚
è¯¥å› å­çš„ä¼˜åŒ–å†å²ã€‚
æ‚¨çš„ç›®æ ‡æ˜¯ä¼˜åŒ–å› å­è¡¨è¾¾å¼ï¼Œæå‡å…¶æ€§èƒ½ã€‚

å¯ç”¨æ•°æ®å­—æ®µ
æ‚¨å¯ä½¿ç”¨ä»¥ä¸‹æ•°æ®å­—æ®µæ„å»ºè¡¨è¾¾å¼ã€‚è¯·ç¡®ä¿åœ¨è¡¨è¾¾å¼ä¸­ç›´æ¥ä½¿ç”¨è¿™äº›å­—æ®µåï¼š
- close: æ”¶ç›˜ä»·
- open: å¼€ç›˜ä»·
- high: æœ€é«˜ä»·
- low: æœ€ä½ä»·
- volume: æˆäº¤é‡

å¯ç”¨è¿ç®—ç¬¦
æ‚¨å¯ä½¿ç”¨pandasçš„æ ‡å‡†æ“ä½œç¬¦å’Œå‡½æ•°ï¼Œå¦‚pct_changeã€rollingã€meanã€stdã€diffç­‰

ç°æœ‰å› å­ä¿¡æ¯
å› å­è¡¨è¾¾å¼: {factor_expression}
å› å­è§£é‡Š: {factor_explanation}
è¯„ä¼°ç»“æœ: {evaluation_results}

ä¼˜åŒ–å†å²
ä¼˜åŒ–å†å²ï¼šæš‚æ— 

è¦æ±‚
ä¼˜è´¨å› å­åº”æ»¡è¶³ï¼š
æ’åºä¿¡æ¯ç³»æ•°ï¼ˆRankICï¼‰> 0.015
æ’åºä¿¡æ¯æ¯”ï¼ˆRankIRï¼‰> 0.2
æ¢æ‰‹ç‡ï¼ˆTurnoverï¼‰< 1.5
å¤šæ ·æ€§ï¼ˆDiversityï¼‰> 0.2
æ‚¨çš„ç›®æ ‡æ˜¯åŸºäºä¸Šè¿°æ€§èƒ½æŒ‡æ ‡ä¼˜åŒ–å› å­ã€‚é€šå¸¸ï¼Œæå‡æ’åºä¿¡æ¯ç³»æ•°ï¼ˆRankICï¼‰ä¸æ’åºä¿¡æ¯æ¯”ï¼ˆRankIRï¼‰æ˜¯é¦–è¦ç›®æ ‡ï¼Œæ¢æ‰‹ç‡ä¸å¤šæ ·æ€§ä¸ºæ¬¡è¦ç›®æ ‡ã€‚
å› å­åº”ç”Ÿæˆæ— é‡çº²å€¼ï¼Œä¸å—ä»·æ ¼å°ºåº¦æˆ–æˆäº¤é‡å•ä½çš„å½±å“ã€‚
ä»…ä½¿ç”¨æä¾›çš„æ•°æ®å­—æ®µä¸è¿ç®—ç¬¦ã€‚
æ— éœ€ä½¿ç”¨æ‰€æœ‰æ•°æ®å­—æ®µï¼Œå› å­è¡¨è¾¾å¼ä¸­ä½¿ç”¨çš„æ•°æ®å­—æ®µæ€»æ•°ä¸è¶…è¿‡ 3 ä¸ªã€‚
é¿å…è¿‡åº¦åµŒå¥—è¿ç®—ç¬¦æˆ–æ„å»ºè¿‡äºå¤æ‚çš„è¡¨è¾¾å¼ï¼Œä»¥é™ä½è¿‡æ‹Ÿåˆé£é™©ã€‚
è¡¨è¾¾å¼åº”ç®€æ´ã€æ˜“è¯»ä¸”å¯è§£é‡Šã€‚
é¿å…é‡å¤å…ˆå‰çš„å°è¯•ã€‚
æ£€æŸ¥å› å­è¡¨è¾¾å¼ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚

è¾“å‡ºæ ¼å¼
è¯·è¿”å›ä»¥ä¸‹æ ¼å¼ï¼š
ä¼˜åŒ–åè¡¨è¾¾å¼: your_optimized_expression
æ”¹è¿›è¯´æ˜: your_improvement_explanation
            """,
            
            "complementary_factor_generation": """
ç›®æ ‡
æ‚¨æ˜¯é‡åŒ–äº¤æ˜“é˜¿å°”æ³•å› å­ç”Ÿæˆé¢†åŸŸçš„ä¸“å®¶ã€‚æ‚¨çš„ä»»åŠ¡æ˜¯è®¾è®¡ä¸€ä¸ªä¸ç°æœ‰å› å­äº’è¡¥çš„æ–°æ¨ªæˆªé¢é˜¿å°”æ³•å› å­â€”â€” è¿™æ˜¯ä¸€ç§æ•°å­¦è¡¨è¾¾å¼ï¼Œèƒ½å¤ŸåŸºäºè‚¡ç¥¨è¿‘æœŸçš„å¸‚åœºæ•°æ®ï¼Œåœ¨æ¯ä¸ªäº¤æ˜“æ—¥ä¸ºæ¯åªè‚¡ç¥¨åˆ†é…ä¸€ä¸ªåˆ†æ•°ã€‚è¯¥åˆ†æ•°å°†ç”¨äºå½“æ—¥å¯¹è‚¡ç¥¨è¿›è¡Œæ’åä¸ç­›é€‰ï¼ˆå³è·¨å¸‚åœºæ¨ªæˆªé¢æ“ä½œï¼‰ã€‚
æ‚¨å°†è·å¾—ï¼š
å¯ç”¨æ•°æ®å­—æ®µä¸è¿ç®—ç¬¦åˆ—è¡¨ã€‚
ç°æœ‰æœ‰æ•ˆå› å­çš„é›†åˆåŠå…¶æ€§èƒ½ã€‚
æ‚¨çš„ç›®æ ‡æ˜¯ç”Ÿæˆä¸€ä¸ªé€»è¾‘ä¸Šä¸ç°æœ‰å› å­äº’è¡¥çš„æ–°å› å­ï¼Œæ•æ‰ä¸åŒçš„å¸‚åœºç‰¹å¾ã€‚

å¯ç”¨æ•°æ®å­—æ®µ
æ‚¨å¯ä½¿ç”¨ä»¥ä¸‹æ•°æ®å­—æ®µæ„å»ºè¡¨è¾¾å¼ã€‚è¯·ç¡®ä¿åœ¨è¡¨è¾¾å¼ä¸­ç›´æ¥ä½¿ç”¨è¿™äº›å­—æ®µåï¼š
- close: æ”¶ç›˜ä»·
- open: å¼€ç›˜ä»·
- high: æœ€é«˜ä»·
- low: æœ€ä½ä»·
- volume: æˆäº¤é‡

å¯ç”¨è¿ç®—ç¬¦
æ‚¨å¯ä½¿ç”¨pandasçš„æ ‡å‡†æ“ä½œç¬¦å’Œå‡½æ•°ï¼Œå¦‚pct_changeã€rollingã€meanã€stdã€diffç­‰

ç°æœ‰å› å­åˆ†æ
ç°æœ‰æœ‰æ•ˆå› å­ï¼š
{effective_factors}

ç°æœ‰å› å­è¡¨è¾¾å¼ç¤ºä¾‹ï¼ˆé¿å…é‡å¤ï¼‰ï¼š
{existing_expressions}

è¦æ±‚
ç”Ÿæˆçš„å› å­åº”ä¸ç°æœ‰å› å­åœ¨é€»è¾‘ä¸Šäº’è¡¥ï¼Œæ•æ‰ä¸åŒçš„å¸‚åœºç‰¹å¾ï¼š
- å¦‚æœç°æœ‰å› å­ä¸»è¦æ˜¯åŠ¨é‡ç±»å› å­ï¼Œä¼˜å…ˆç”Ÿæˆå‡å€¼å›å½’ç±»å› å­
- å¦‚æœç°æœ‰å› å­ä¸»è¦æ˜¯è¶‹åŠ¿ç±»å› å­ï¼Œä¼˜å…ˆç”Ÿæˆæ³¢åŠ¨ç‡ç±»å› å­
- å¦‚æœç°æœ‰å› å­ä¸»è¦æ˜¯ä»·æ ¼ç±»å› å­ï¼Œä¼˜å…ˆç”Ÿæˆæˆäº¤é‡ç±»å› å­
å› å­åº”ç”Ÿæˆæ— é‡çº²å€¼ï¼Œä¸å—ä»·æ ¼å°ºåº¦æˆ–æˆäº¤é‡å•ä½çš„å½±å“ã€‚
ä»…ä½¿ç”¨æä¾›çš„æ•°æ®å­—æ®µä¸è¿ç®—ç¬¦ã€‚
æ— éœ€ä½¿ç”¨æ‰€æœ‰æ•°æ®å­—æ®µï¼Œå› å­è¡¨è¾¾å¼ä¸­ä½¿ç”¨çš„æ•°æ®å­—æ®µæ€»æ•°ä¸è¶…è¿‡ 3 ä¸ªã€‚
é¿å…è¿‡åº¦åµŒå¥—è¿ç®—ç¬¦æˆ–æ„å»ºè¿‡äºå¤æ‚çš„è¡¨è¾¾å¼ï¼Œä»¥é™ä½è¿‡æ‹Ÿåˆé£é™©ã€‚
è¡¨è¾¾å¼åº”ç®€æ´ã€æ˜“è¯»ä¸”å¯è§£é‡Šã€‚
é¿å…é‡å¤ç°æœ‰å› å­çš„æ ¸å¿ƒé€»è¾‘å’Œæ¨¡å¼ã€‚

è¾“å‡ºæ ¼å¼
è¯·è¿”å›ä»¥ä¸‹æ ¼å¼ï¼š
è¡¨è¾¾å¼: your_factor_expression
è§£é‡Š: your_factor_explanation
            """
        }
        return templates
    
    def _init_llm(self):
        """
        åˆå§‹åŒ–LLMå®ä¾‹
        """
        # å¦‚æœæ˜¯æ¨¡æ‹Ÿæ¨¡å¼ï¼Œç›´æ¥è¿”å›None
        if hasattr(self, 'mock_mode') and self.mock_mode:
            return None
            
        try:
            # å°è¯•å¯¼å…¥LangChain OpenAIé›†æˆ
            from langchain_openai import ChatOpenAI
            
            # è·å–APIå¯†é’¥
            api_key = os.environ.get("OPENAI_API_KEY")
            base_url = os.environ.get("OPENAI_BASE_URL")
            
            # æ£€æŸ¥APIå¯†é’¥
            if not api_key:
                logger.warning("âš ï¸ OPENAI_API_KEYç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
                self.mock_mode = True
                return None
            
            # åˆ›å»ºLLMå®ä¾‹
            llm = ChatOpenAI(
                model=self.model_name,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                api_key=api_key,
                base_url=base_url if base_url else None
            )
            
            return llm
        except ImportError:
            # å°è¯•ä½¿ç”¨å…¶ä»–LLMæä¾›å•†
            try:
                # å¯ä»¥æ·»åŠ å…¶ä»–æ¨¡å‹å¦‚DeepSeekã€é˜¿é‡Œç™¾ç‚¼ç­‰
                logger.warning(f"âš ï¸  OpenAIé›†æˆä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨å¤‡ç”¨æ¨¡å‹")
                
                # å¦‚æœæ²¡æœ‰å¯ç”¨çš„LLMï¼Œä½¿ç”¨æ¨¡æ‹Ÿå“åº”
                from langchain_core.language_models import FakeListChatModel
                
                # é¢„å®šä¹‰çš„æ¨¡æ‹Ÿå“åº”
                responses = [
                    "è¡¨è¾¾å¼: close.pct_change(10).rolling(window=5).mean() * volume.pct_change(5).rolling(window=10).mean()\nè§£é‡Š: ç»“åˆåŠ¨é‡å’Œæˆäº¤é‡å˜åŒ–çš„å¤åˆå› å­ï¼Œæ•æ‰ä»·æ ¼å’Œæˆäº¤é‡çš„ååŒæ•ˆåº”ã€‚",
                    "è¡¨è¾¾å¼: (close / close.rolling(window=50).mean() - 1) * (close - low) / (high - low + 1e-8)\nè§£é‡Š: ç»“åˆç›¸å¯¹å¼ºå¼±å’Œæ—¥å†…æ³¢åŠ¨ç‰¹æ€§çš„å› å­ï¼Œæ•æ‰è¶…ä¹°è¶…å–çŠ¶æ€ã€‚",
                    "è¡¨è¾¾å¼: close.pct_change().rolling(window=20).std() / close.pct_change().rolling(window=60).std()\nè§£é‡Š: çŸ­æœŸæ³¢åŠ¨ç‡ä¸é•¿æœŸæ³¢åŠ¨ç‡çš„æ¯”ç‡ï¼Œæ•æ‰å¸‚åœºæƒ…ç»ªå˜åŒ–ã€‚"
                ]
                
                return FakeListChatModel(responses=responses)
            except Exception as e:
                logger.error(f"âŒ åˆå§‹åŒ–LLMå¤±è´¥: {e}")
                # å¤±è´¥æ—¶å¯ç”¨æ¨¡æ‹Ÿæ¨¡å¼
                self.mock_mode = True
                logger.warning("âš ï¸ åˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ¨¡å¼")
                return None
    
    def generate_factor_expression(self, 
                                effective_factors: List[Dict[str, Any]] = None,
                                discarded_factors: List[Dict[str, Any]] = None) -> Tuple[str, str]:
        """
        ä½¿ç”¨LLMç”Ÿæˆæ–°çš„å› å­è¡¨è¾¾å¼
        
        Args:
            effective_factors: æœ‰æ•ˆå› å­åˆ—è¡¨ä½œä¸ºæ­£å‘å‚è€ƒ
            discarded_factors: åºŸå¼ƒå› å­åˆ—è¡¨ä½œä¸ºè´Ÿå‘å‚è€ƒ
            
        Returns:
            (å› å­è¡¨è¾¾å¼, å› å­è§£é‡Š)
        """
        if effective_factors is None:
            effective_factors = []
        if discarded_factors is None:
            discarded_factors = []
        
        # æ ¼å¼åŒ–å‚è€ƒå› å­
        effective_factors_str = "\n".join([
            f"- {f.get('name', 'Unknown')}: {f.get('expression', 'Unknown')} (IC: {f.get('evaluation_metrics', {}).get('ic', 0):.4f})" 
            for f in effective_factors
        ])
        
        discarded_factors_str = "\n".join([
            f"- {f.get('name', 'Unknown')}: {f.get('expression', 'Unknown')} (IC: {f.get('evaluation_metrics', {}).get('ic', 0):.4f}, åŸå› : {f.get('reason', 'Unknown')})" 
            for f in discarded_factors
        ])
        
        # å¦‚æœæ²¡æœ‰å‚è€ƒå› å­ï¼Œä½¿ç”¨é»˜è®¤æ–‡æœ¬
        if not effective_factors_str:
            effective_factors_str = "æš‚æ— æœ‰æ•ˆå‚è€ƒå› å­"
        if not discarded_factors_str:
            discarded_factors_str = "æš‚æ— åºŸå¼ƒå‚è€ƒå› å­"
        
        # æ„å»ºæç¤ºè¯
        prompt = self.prompt_templates["factor_generation"].format(
            effective_factors=effective_factors_str,
            discarded_factors=discarded_factors_str
        )
        
        logger.info(f"ğŸ“ è°ƒç”¨LLMç”Ÿæˆå› å­è¡¨è¾¾å¼")
        
        # è°ƒç”¨LLM
        try:
            # æ£€æŸ¥æ˜¯å¦åœ¨æ¨¡æ‹Ÿæ¨¡å¼
            if hasattr(self, 'mock_mode') and self.mock_mode:
                # æ¨¡æ‹Ÿå“åº”
                mock_responses = [
                    "è¡¨è¾¾å¼: close.pct_change(20).rolling(window=10).mean()\nè§£é‡Š: åŸºäº20æ—¥ä»·æ ¼å˜åŒ–çš„ç§»åŠ¨å¹³å‡å› å­ï¼Œæ•æ‰ä¸­çŸ­æœŸä»·æ ¼åŠ¨é‡ã€‚",
                    "è¡¨è¾¾å¼: (close - low) / (high - low + 1e-8) * volume.pct_change(5)\nè§£é‡Š: ç»“åˆä»·æ ¼ä½ç½®å’Œæˆäº¤é‡å˜åŒ–çš„å› å­ï¼Œæ•æ‰ä¹°å–åŠ›é‡å˜åŒ–ã€‚",
                    "è¡¨è¾¾å¼: close.pct_change().rolling(window=20).std()\nè§£é‡Š: 20æ—¥ä»·æ ¼æ³¢åŠ¨ç‡å› å­ï¼Œæ•æ‰å¸‚åœºä¸ç¡®å®šæ€§ã€‚"
                ]
                response = random.choice(mock_responses)
                logger.info(f"â„¹ï¸  ä½¿ç”¨æ¨¡æ‹Ÿå“åº”: {response}")
            else:
                # è°ƒç”¨å¯¹åº”çš„LLMé€‚é…å™¨ç”ŸæˆæŠ¥å‘Š
                response = self.llm_adapter.generate_report_section(
                    system_prompt="ä½ æ˜¯ä¸€åä¸“ä¸šçš„é‡åŒ–å› å­è®¾è®¡ä¸“å®¶ã€‚",
                    user_prompt=prompt
                )
            
            # è§£æå“åº”
            expression_match = re.search(r'è¡¨è¾¾å¼:\s*(.*)', response, re.MULTILINE)
            explanation_match = re.search(r'è§£é‡Š:\s*(.*)', response, re.MULTILINE)
            
            if expression_match and explanation_match:
                expression = expression_match.group(1).strip()
                explanation = explanation_match.group(1).strip()
                
                logger.info(f"âœ… å› å­ç”ŸæˆæˆåŠŸ")
                logger.info(f"ğŸ“Š è¡¨è¾¾å¼: {expression}")
                logger.info(f"ğŸ“Š è§£é‡Š: {explanation}")
                
                return expression, explanation
            else:
                logger.error(f"âŒ æ— æ³•è§£æLLMå“åº”: {response}")
                raise ValueError("æ— æ³•ä»LLMå“åº”ä¸­æå–å› å­è¡¨è¾¾å¼å’Œè§£é‡Š")
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå› å­è¡¨è¾¾å¼å¤±è´¥: {e}")
            # è¿”å›å¤‡ç”¨è¡¨è¾¾å¼
            fallback_expression = f"close.pct_change({random.randint(3, 20)}).rolling(window={random.randint(3, 10)}).mean()"
            fallback_explanation = "å¤‡ç”¨åŠ¨é‡å› å­ï¼Œä½¿ç”¨éšæœºå‚æ•°"
            logger.warning(f"âš ï¸ ä½¿ç”¨å¤‡ç”¨å› å­: {fallback_expression}")
            return fallback_expression, fallback_explanation
                
    def generate_complementary_factor(self, 
                                    effective_factors: List[Dict[str, Any]] = None,
                                    existing_expressions: List[str] = None) -> Tuple[str, str]:
        """
        ä½¿ç”¨LLMç”Ÿæˆä¸ç°æœ‰å› å­äº’è¡¥çš„æ–°å› å­è¡¨è¾¾å¼
        
        Args:
            effective_factors: æœ‰æ•ˆå› å­åˆ—è¡¨ï¼Œç”¨äºåˆ†æç°æœ‰å› å­ç‰¹ç‚¹
            existing_expressions: ç°æœ‰å› å­è¡¨è¾¾å¼åˆ—è¡¨ï¼Œç”¨äºé¿å…é‡å¤
            
        Returns:
            (å› å­è¡¨è¾¾å¼, å› å­è§£é‡Š)
        """
        if effective_factors is None:
            effective_factors = []
        if existing_expressions is None:
            existing_expressions = []
        
        # æ ¼å¼åŒ–ç°æœ‰æœ‰æ•ˆå› å­
        effective_factors_str = "\n".join([
            f"- {f.get('name', 'Unknown')}: {f.get('expression', 'Unknown')} (IC: {f.get('evaluation_metrics', {}).get('ic', 0):.4f}, Sharpe: {f.get('evaluation_metrics', {}).get('sharpe', 0):.4f})" 
            for f in effective_factors[:5]  # åªæ˜¾ç¤ºå‰5ä¸ªå› å­
        ])
        
        # æ ¼å¼åŒ–ç°æœ‰è¡¨è¾¾å¼
        existing_expressions_str = "\n".join([
            f"- {expr[:100]}..." if len(expr) > 100 else f"- {expr}" 
            for expr in existing_expressions[:5]  # åªæ˜¾ç¤ºå‰5ä¸ªè¡¨è¾¾å¼
        ])
        
        # å¦‚æœæ²¡æœ‰å› å­æˆ–è¡¨è¾¾å¼ï¼Œä½¿ç”¨é»˜è®¤æ–‡æœ¬
        if not effective_factors_str:
            effective_factors_str = "æš‚æ— æœ‰æ•ˆå‚è€ƒå› å­"
        if not existing_expressions_str:
            existing_expressions_str = "æš‚æ— ç°æœ‰è¡¨è¾¾å¼"
        
        # æ„å»ºæç¤ºè¯
        prompt = self.prompt_templates["complementary_factor_generation"].format(
            effective_factors=effective_factors_str,
            existing_expressions=existing_expressions_str
        )
        
        logger.info(f"ğŸ“ è°ƒç”¨LLMç”Ÿæˆäº’è¡¥å› å­è¡¨è¾¾å¼")
        
        # è°ƒç”¨LLM
        try:
            # æ£€æŸ¥æ˜¯å¦åœ¨æ¨¡æ‹Ÿæ¨¡å¼
            if hasattr(self, 'mock_mode') and self.mock_mode:
                # æ¨¡æ‹Ÿå“åº” - æä¾›äº’è¡¥æ€§å¼ºçš„å› å­
                mock_responses = [
                    "è¡¨è¾¾å¼: (close - close.rolling(window=20).mean()) / close.rolling(window=20).std()\nè§£é‡Š: åŸºäºZ-scoreçš„å‡å€¼å›å½’å› å­ï¼Œä¸åŠ¨é‡å› å­å½¢æˆäº’è¡¥ã€‚",
                    "è¡¨è¾¾å¼: volume.pct_change(5).rolling(window=10).std()\nè§£é‡Š: æˆäº¤é‡æ³¢åŠ¨ç‡å› å­ï¼Œæ•æ‰æµåŠ¨æ€§å˜åŒ–ï¼Œä¸ä»·æ ¼ç±»å› å­å½¢æˆäº’è¡¥ã€‚",
                    "è¡¨è¾¾å¼: (high - close) / (high - low + 1e-8)\nè§£é‡Š: æ—¥å†…åè½¬å› å­ï¼Œæ•æ‰çŸ­æœŸä»·æ ¼åè½¬ç‰¹å¾ï¼Œä¸è¶‹åŠ¿è·Ÿè¸ªå› å­å½¢æˆäº’è¡¥ã€‚"
                ]
                response = random.choice(mock_responses)
                logger.info(f"â„¹ï¸  ä½¿ç”¨æ¨¡æ‹Ÿå“åº”: {response}")
            else:
                # è°ƒç”¨å¯¹åº”çš„LLMé€‚é…å™¨ç”ŸæˆæŠ¥å‘Š
                response = self.llm_adapter.generate_report_section(
                    system_prompt="ä½ æ˜¯ä¸€åä¸“ä¸šçš„é‡åŒ–å› å­è®¾è®¡ä¸“å®¶ï¼Œç‰¹åˆ«æ“…é•¿è®¾è®¡ä¸ç°æœ‰å› å­äº’è¡¥çš„æ–°å› å­ã€‚",
                    user_prompt=prompt
                )
            
            # è§£æå“åº”
            expression_match = re.search(r'è¡¨è¾¾å¼:\s*(.*)', response, re.MULTILINE)
            explanation_match = re.search(r'è§£é‡Š:\s*(.*)', response, re.MULTILINE)
            
            if expression_match and explanation_match:
                expression = expression_match.group(1).strip()
                explanation = explanation_match.group(1).strip()
                
                logger.info(f"âœ… äº’è¡¥å› å­ç”ŸæˆæˆåŠŸ")
                logger.info(f"ğŸ“Š è¡¨è¾¾å¼: {expression}")
                logger.info(f"ğŸ“Š è§£é‡Š: {explanation}")
                
                return expression, explanation
            else:
                logger.error(f"âŒ æ— æ³•è§£æLLMå“åº”: {response}")
                raise ValueError("æ— æ³•ä»LLMå“åº”ä¸­æå–å› å­è¡¨è¾¾å¼å’Œè§£é‡Š")
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆäº’è¡¥å› å­è¡¨è¾¾å¼å¤±è´¥: {e}")
            # è¿”å›å¤‡ç”¨äº’è¡¥å› å­è¡¨è¾¾å¼
            fallback_expression = "(close - close.rolling(window=20).mean()) / close.rolling(window=20).std()"
            fallback_explanation = "å¤‡ç”¨å‡å€¼å›å½’å› å­ï¼Œä¸åŠ¨é‡ç±»å› å­å½¢æˆäº’è¡¥"
            logger.warning(f"âš ï¸ ä½¿ç”¨å¤‡ç”¨äº’è¡¥å› å­: {fallback_expression}")
            return fallback_expression, fallback_explanation
    
    def optimize_factor_expression(self, 
                                 factor_expression: str,
                                 factor_explanation: str,
                                 evaluation_results: Dict[str, float],
                                 improvement_suggestions: str) -> Tuple[str, str]:
        """
        ä½¿ç”¨LLMä¼˜åŒ–ç°æœ‰å› å­è¡¨è¾¾å¼
        
        Args:
            factor_expression: åŸå§‹å› å­è¡¨è¾¾å¼
            factor_explanation: åŸå§‹å› å­è§£é‡Š
            evaluation_results: è¯„ä¼°ç»“æœ
            improvement_suggestions: æ”¹è¿›å»ºè®®
            
        Returns:
            (ä¼˜åŒ–åçš„å› å­è¡¨è¾¾å¼, æ”¹è¿›è¯´æ˜)
        """
        # æ ¼å¼åŒ–è¯„ä¼°ç»“æœ
        eval_results_str = "\n".join([
            f"- {key}: {value:.4f}" for key, value in evaluation_results.items()
        ])
        
        # æ„å»ºæç¤ºè¯
        prompt = self.prompt_templates["factor_optimization"].format(
            factor_expression=factor_expression,
            factor_explanation=factor_explanation,
            evaluation_results=eval_results_str,
            improvement_suggestions=improvement_suggestions
        )
        
        logger.info(f"ğŸ“ è°ƒç”¨LLMä¼˜åŒ–å› å­è¡¨è¾¾å¼")
        
        # è°ƒç”¨LLM
        try:
            # æ£€æŸ¥æ˜¯å¦åœ¨æ¨¡æ‹Ÿæ¨¡å¼
            if hasattr(self, 'mock_mode') and self.mock_mode:
                # æ¨¡æ‹Ÿä¼˜åŒ–å“åº”
                mock_responses = [
                    f"ä¼˜åŒ–åè¡¨è¾¾å¼: {factor_expression.replace('20', '15').replace('10', '8')}\næ”¹è¿›è¯´æ˜: è°ƒæ•´äº†æ—¶é—´çª—å£å‚æ•°ï¼Œä½¿å› å­å¯¹å¸‚åœºå˜åŒ–æ›´æ•æ„Ÿã€‚",
                    f"ä¼˜åŒ–åè¡¨è¾¾å¼: ({factor_expression}) * volume.pct_change(5).rolling(window=10).mean()\næ”¹è¿›è¯´æ˜: ç»“åˆæˆäº¤é‡å˜åŒ–æé«˜å› å­çš„æœ‰æ•ˆæ€§ã€‚",
                    f"ä¼˜åŒ–åè¡¨è¾¾å¼: {factor_expression}.rolling(window=5).mean()\næ”¹è¿›è¯´æ˜: æ·»åŠ äº†å¹³æ»‘å¤„ç†ï¼Œå‡å°‘å› å­å™ªéŸ³ã€‚"
                ]
                response = random.choice(mock_responses)
                logger.info(f"â„¹ï¸  ä½¿ç”¨æ¨¡æ‹Ÿä¼˜åŒ–å“åº”: {response}")
            else:
                # è°ƒç”¨å¯¹åº”çš„LLMé€‚é…å™¨ç”ŸæˆæŠ¥å‘Š
                response = self.llm_adapter.generate_report_section(
                    system_prompt="ä½ æ˜¯ä¸€åä¸“ä¸šçš„é‡åŒ–å› å­ä¼˜åŒ–ä¸“å®¶ã€‚",
                    user_prompt=prompt
                )
            
            # è§£æå“åº”
            expression_match = re.search(r'ä¼˜åŒ–åè¡¨è¾¾å¼:\s*(.*)', response, re.MULTILINE)
            explanation_match = re.search(r'æ”¹è¿›è¯´æ˜:\s*(.*)', response, re.MULTILINE)
            
            if expression_match and explanation_match:
                optimized_expression = expression_match.group(1).strip()
                improvement_explanation = explanation_match.group(1).strip()
                
                logger.info(f"âœ… å› å­ä¼˜åŒ–æˆåŠŸ")
                logger.info(f"ğŸ“Š ä¼˜åŒ–åè¡¨è¾¾å¼: {optimized_expression}")
                logger.info(f"ğŸ“Š æ”¹è¿›è¯´æ˜: {improvement_explanation}")
                
                return optimized_expression, improvement_explanation
            else:
                logger.error(f"âŒ æ— æ³•è§£æLLMå“åº”: {response}")
                raise ValueError("æ— æ³•ä»LLMå“åº”ä¸­æå–ä¼˜åŒ–åçš„è¡¨è¾¾å¼å’Œè¯´æ˜")
        except Exception as e:
            logger.error(f"âŒ ä¼˜åŒ–å› å­è¡¨è¾¾å¼å¤±è´¥: {e}")
            # è¿”å›åŸå§‹è¡¨è¾¾å¼
            logger.warning(f"âš ï¸ ä¼˜åŒ–å¤±è´¥ï¼Œè¿”å›åŸå§‹å› å­")
            return factor_expression, "ä¼˜åŒ–å¤±è´¥ï¼Œä¿ç•™åŸå§‹å› å­"
    
    def validate_factor_expression(self, expression: str) -> Tuple[bool, str]:
        """
        éªŒè¯å› å­è¡¨è¾¾å¼çš„æœ‰æ•ˆæ€§
        
        Args:
            expression: å› å­è¡¨è¾¾å¼
            
        Returns:
            (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯ä¿¡æ¯)
        """
        # åŸºæœ¬è¯­æ³•æ£€æŸ¥
        try:
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•ç¯å¢ƒ
            import pandas as pd
            import numpy as np
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            dates = pd.date_range('2020-01-01', periods=10)
            stocks = ['stock1', 'stock2']
            index = pd.MultiIndex.from_product([dates, stocks], names=['date', 'stock'])
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®æ¡†
            test_data = pd.DataFrame({
                'close': np.random.random(20),
                'open': np.random.random(20),
                'high': np.random.random(20),
                'low': np.random.random(20),
                'volume': np.random.random(20) * 1000
            }, index=index)
            
            # å°è¯•ç¼–è¯‘è¡¨è¾¾å¼
            compiled = compile(expression, '<string>', 'eval')
            
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†å…è®¸çš„å˜é‡
            allowed_vars = {'close', 'open', 'high', 'low', 'volume', 'np', 'pd'}
            for name in compiled.co_names:
                if name not in allowed_vars and not hasattr(pd.Series, name) and not hasattr(np, name):
                    return False, f"ä¸å…è®¸ä½¿ç”¨çš„å˜é‡: {name}"
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ½œåœ¨çš„å±é™©æ“ä½œ
            dangerous_patterns = [
                r'__[^_]+__',  # åŒä¸‹åˆ’çº¿æ–¹æ³•
                r'import\s+',  # importè¯­å¥
                r'exec\s*\(',  # execå‡½æ•°
                r'eval\s*\(',  # evalå‡½æ•°
                r'open\s*\(',  # openå‡½æ•°
                r'file\s*\(',  # fileå‡½æ•°
                r'compile\s*\(',  # compileå‡½æ•°
            ]
            
            for pattern in dangerous_patterns:
                if re.search(pattern, expression, re.IGNORECASE):
                    return False, f"æ½œåœ¨çš„å±é™©æ“ä½œ: {pattern}"
            
            return True, "è¡¨è¾¾å¼æœ‰æ•ˆ"
        except SyntaxError as e:
            return False, f"è¯­æ³•é”™è¯¯: {e}"
        except Exception as e:
            return False, f"éªŒè¯å¤±è´¥: {e}"
    
    def generate_improvement_suggestions(self, evaluation_results: Dict[str, float]) -> str:
        """
        æ ¹æ®è¯„ä¼°ç»“æœç”Ÿæˆæ”¹è¿›å»ºè®®
        
        Args:
            evaluation_results: è¯„ä¼°ç»“æœ
            
        Returns:
            æ”¹è¿›å»ºè®®
        """
        suggestions = []
        
        # åŸºäºICçš„å»ºè®®
        if evaluation_results["ic"] < 0.01:
            suggestions.append("æé«˜å› å­ä¸æœªæ¥æ”¶ç›Šç‡çš„ç›¸å…³æ€§ï¼Œå¯ä»¥å°è¯•è°ƒæ•´æ—¶é—´çª—å£æˆ–ç»“åˆå…¶ä»–æ•°æ®æº")
        elif evaluation_results["ic"] < 0.02:
            suggestions.append("ICå€¼æœ‰æå‡ç©ºé—´ï¼Œå¯ä»¥è€ƒè™‘æ”¹è¿›å› å­çš„æ—¶é—´åºåˆ—ç‰¹æ€§")
        
        # åŸºäºå¤æ™®æ¯”ç‡çš„å»ºè®®
        if evaluation_results["sharpe"] < 0.5:
            suggestions.append("é™ä½å› å­çš„æ³¢åŠ¨æ€§ï¼Œå¯ä»¥å°è¯•æ·»åŠ å¹³æ»‘å¤„ç†æˆ–è°ƒæ•´æƒé‡")
        
        # åŸºäºæ”¶ç›Šç‡çš„å»ºè®®
        if evaluation_results["annual_return"] < 0:
            suggestions.append("å› å­æ”¶ç›Šä¸ºè´Ÿï¼Œéœ€è¦é‡æ–°è®¾è®¡å› å­é€»è¾‘")
        elif evaluation_results["annual_return"] < 0.1:
            suggestions.append("å°è¯•æé«˜å› å­çš„æ”¶ç›Šæ½œåŠ›ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´å‚æ•°æˆ–ç»“åˆéçº¿æ€§å˜æ¢")
        
        # ç»¼åˆå»ºè®®
        if not suggestions:
            suggestions.append("å› å­è¡¨ç°è‰¯å¥½ï¼Œå¯ä»¥å°è¯•å¾®è°ƒå‚æ•°æˆ–æ·»åŠ æ›´å¤šç‰¹å¾")
        
        return "\n".join(suggestions)