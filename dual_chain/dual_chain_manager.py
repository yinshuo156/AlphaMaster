# -*- coding: utf-8 -*-
"""
åŒé“¾ååŒç®¡ç†å™¨
å®ç°å› å­ç”Ÿæˆã€è¯„ä¼°ã€ä¼˜åŒ–å’Œæ›´æ–°å› å­æ± çš„è‡ªåŠ¨åŒ–é—­ç¯æµç¨‹
"""

import os
import json
import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime
import importlib.util

from dual_chain.factor_pool import FactorPool
from dual_chain.factor_evaluator import FactorEvaluator
from dual_chain.llm_factor_adapter import LLMFactorAdapter

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('dual_chain.dual_chain_manager')

class DualChainManager:
    """
    åŒé“¾ååŒç®¡ç†å™¨
    å®ç°"ç”Ÿæˆâ†’è¯„ä¼°â†’ä¼˜åŒ–â†’å†è¯„ä¼°â†’æ›´æ–°å› å­æ± "çš„è‡ªåŠ¨åŒ–é—­ç¯æµç¨‹
    """
    
    def __init__(self, 
                 data_path: str = "data/a_share",
                 pool_dir: str = "dual_chain/pools",
                 llm_model: str = "gpt-4",
                 llm_provider: str = None,
                 llm_api_key: str = None,
                 alpha_master_dir: str = None,
                 mock_mode: bool = False):
        """
        åˆå§‹åŒ–åŒé“¾ååŒç®¡ç†å™¨
        
        Args:
            data_path: æ•°æ®è·¯å¾„
            pool_dir: å› å­æ± ç›®å½•
            llm_model: LLMæ¨¡å‹åç§°
            llm_provider: LLMæä¾›å•†ï¼Œæ”¯æŒ'openai'ã€'dashscope'(é˜¿é‡Œç™¾ç‚¼)ã€'deepseek'ç­‰
            llm_api_key: APIå¯†é’¥
            alpha_master_dir: Alpha Masterç›®å½•è·¯å¾„
            mock_mode: æ˜¯å¦ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼ï¼Œåœ¨æ¨¡æ‹Ÿæ¨¡å¼ä¸‹å°†ä½¿ç”¨é¢„è®¾çš„å› å­å’Œæ•°æ®è¿›è¡Œæµ‹è¯•
        """
        self.logger = logging.getLogger('dual_chain.dual_chain_manager')
        self.data_path = data_path
        self.pool_dir = pool_dir
        self.output_dir = os.path.join(pool_dir, "output")
        self.alpha_master_dir = alpha_master_dir
        self.mock_mode = mock_mode
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.factor_pool = FactorPool(pool_dir)
        self.data_loader = self._init_data_loader()
        self.returns_data = self._get_returns_data()
        self.evaluator = FactorEvaluator(self.returns_data)
        self.llm_adapter = LLMFactorAdapter(
            model_name=llm_model,
            provider=llm_provider,
            api_key=llm_api_key
        )
        
        # åˆå§‹åŒ–ç”Ÿæˆå™¨
        self.factor_generator = self._init_factor_generator()
        
        logger.info(f"âœ… åŒé“¾ååŒç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
    
    def _init_data_loader(self):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        """
        # å°è¯•ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®åŠ è½½å™¨
        try:
            # å¯¼å…¥è‡ªå®šä¹‰æ•°æ®åŠ è½½å™¨
            import sys
            sys.path.append(os.path.dirname(os.path.dirname(__file__)))
            from aaa.custom_data_loader import CustomDataLoader
            
            logger.info(f"ğŸ“Š ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®åŠ è½½å™¨ï¼Œè·¯å¾„: {self.data_path}")
            return CustomDataLoader(self.data_path)
        except Exception as e:
            logger.warning(f"âš ï¸  å¯¼å…¥è‡ªå®šä¹‰æ•°æ®åŠ è½½å™¨å¤±è´¥: {e}")
        
        # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ›å»ºç®€å•çš„æ•°æ®åŠ è½½å™¨
        logger.info("ğŸ“Š åˆ›å»ºå¤‡ç”¨æ•°æ®åŠ è½½å™¨")
        return self._create_basic_data_loader()
    
    def _create_basic_data_loader(self):
        """
        åˆ›å»ºåŸºç¡€æ•°æ®åŠ è½½å™¨
        """
        class BasicDataLoader:
            def __init__(self, data_path, mock_mode=False):
                self.data_path = data_path
                self.stock_data = {}
                self.mock_mode = mock_mode
                logger.info(f"ğŸ”„ åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨ï¼Œæ•°æ®è·¯å¾„: {data_path}")
                self.load_all_data()
            
            def load_all_data(self):
                import glob
                import os
                logger.info(f"ğŸ“‚ å¼€å§‹æ‰«ææ•°æ®ç›®å½•: {self.data_path}")
                
                # è·å–æ‰€æœ‰CSVæ–‡ä»¶
                csv_files = glob.glob(os.path.join(self.data_path, "*.csv"))
                logger.info(f"ğŸ“„ æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
                
                # æ‰‹åŠ¨æŒ‡å®šè¦å¤„ç†çš„è‚¡ç¥¨æ–‡ä»¶ï¼Œé¿å…å¤„ç†åŸå§‹åˆå¹¶æ–‡ä»¶
                target_files = [f for f in csv_files if os.path.basename(f) not in ['us_share.csv']]
                logger.info(f"ğŸ¯ ç›®æ ‡å¤„ç†æ–‡ä»¶æ•°: {len(target_files)}")
                
                for file_path in target_files:
                    try:
                        logger.info(f"ğŸ” å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path}")
                        
                        # ä»æ–‡ä»¶åæå–è‚¡ç¥¨ä»£ç 
                        file_name = os.path.basename(file_path)
                        symbol = file_name[:-4] if file_name.endswith('.csv') else file_name
                        logger.info(f"ğŸ“ æ–‡ä»¶å: {file_name}, æå–çš„è‚¡ç¥¨ä»£ç : {symbol}")
                        
                        # è¯»å–CSVæ–‡ä»¶
                        logger.info("ğŸ“Š å¼€å§‹è¯»å–CSVæ–‡ä»¶...")
                        df = pd.read_csv(file_path)
                        logger.info(f"âœ… CSVæ–‡ä»¶è¯»å–æˆåŠŸï¼Œå½¢çŠ¶: {df.shape}")
                        logger.info(f"ğŸ“‹ æ•°æ®åˆ—: {list(df.columns)}")
                        
                        # æ£€æŸ¥å¹¶å¤„ç†åˆ—åå·®å¼‚ï¼ˆç¾è‚¡æ•°æ®å¯èƒ½æœ‰ä¸åŒçš„åˆ—åï¼‰
                        # åˆ›å»ºåˆ—åæ˜ å°„å­—å…¸
                        column_mapping = {
                            'open': 'open',
                            'high': 'high',
                            'low': 'low',
                            'close': 'close'
                        }
                        
                        # æŸ¥æ‰¾å¯èƒ½çš„volumeåˆ—å
                        volume_columns = ['volume', 'volume (10K shares)', 'Volume', 'VOLUME']
                        volume_col_found = None
                        for col in volume_columns:
                            if col in df.columns:
                                volume_col_found = col
                                break
                        
                        # æ£€æŸ¥å¿…éœ€çš„ä»·æ ¼åˆ—
                        price_columns = ['open', 'high', 'low', 'close']
                        missing_price_columns = [col for col in price_columns if col not in df.columns]
                        
                        if missing_price_columns:
                            logger.warning(f"âš ï¸  ç¼ºå°‘ä»·æ ¼åˆ—: {missing_price_columns}")
                            # å°è¯•ä½¿ç”¨å…¶ä»–å¯èƒ½çš„åˆ—å
                            alternative_mapping = {
                                'Open': 'open',
                                'High': 'high',
                                'Low': 'low',
                                'Close': 'close',
                                'OPEN': 'open',
                                'HIGH': 'high',
                                'LOW': 'low',
                                'CLOSE': 'close'
                            }
                            
                            for alt_col, std_col in alternative_mapping.items():
                                if alt_col in df.columns and std_col in missing_price_columns:
                                    logger.info(f"ğŸ”„ å°†åˆ— '{alt_col}' æ˜ å°„ä¸º '{std_col}'")
                                    df[std_col] = df[alt_col]
                                    missing_price_columns.remove(std_col)
                        
                        # å¦‚æœä»ç„¶ç¼ºå°‘å¿…è¦çš„ä»·æ ¼åˆ—ï¼Œè·³è¿‡æ­¤æ–‡ä»¶
                        if missing_price_columns:
                            logger.warning(f"âš ï¸  æ— æ³•æ‰¾åˆ°å¿…è¦çš„ä»·æ ¼åˆ—: {missing_price_columns}ï¼Œè·³è¿‡æ­¤æ–‡ä»¶")
                            continue
                        
                        # å¤„ç†æ—¥æœŸ
                        logger.info("ğŸ“… å¤„ç†æ—¥æœŸåˆ—...")
                        df['date'] = pd.to_datetime(df['date'])
                        logger.info(f"âœ… æ—¥æœŸåˆ—å¤„ç†å®Œæˆï¼ŒèŒƒå›´: {df['date'].min()} åˆ° {df['date'].max()}")
                        
                        # å¤„ç†volumeåˆ—
                        if volume_col_found:
                            logger.info(f"ğŸ”¢ å¤„ç†'{volume_col_found}'åˆ—...")
                            vol_col = df[volume_col_found]
                            # å¤„ç†å­—ç¬¦ä¸²ç±»å‹çš„volumeï¼ˆå¯èƒ½åŒ…å«é€—å·ï¼‰
                            if isinstance(vol_col.iloc[0], str):
                                logger.info("ğŸ”„ å°†å­—ç¬¦ä¸²ç±»å‹volumeè½¬æ¢ä¸ºæµ®ç‚¹æ•°")
                                df['volume'] = vol_col.str.replace(',', '').astype(float)
                            else:
                                df['volume'] = vol_col
                        else:
                            logger.warning("âš ï¸  æœªæ‰¾åˆ°volumeåˆ—ï¼Œåˆ›å»ºé»˜è®¤volumeåˆ—")
                            df['volume'] = 1.0
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰symbolåˆ—ï¼Œå¦‚æœæœ‰åˆ™ä½¿ç”¨æ–‡ä»¶ä¸­çš„symbolå€¼
                        if 'symbol' in df.columns:
                            logger.info("ğŸ” å‘ç°æ–‡ä»¶ä¸­åŒ…å«symbolåˆ—")
                            # ä½¿ç”¨æ–‡ä»¶ä¸­çš„symbolåˆ—ä½œä¸ºæ ‡è¯†ï¼Œä½†ä¿ç•™æ–‡ä»¶åä½œä¸ºå­—å…¸é”®
                            logger.info(f"ğŸ“ æ–‡ä»¶ä¸­çš„ç¬¬ä¸€ä¸ªsymbolå€¼: {df['symbol'].iloc[0]}")
                        else:
                            # å¦‚æœæ²¡æœ‰symbolåˆ—ï¼Œæ·»åŠ ä¸€ä¸ª
                            logger.info("ğŸ”„ æ·»åŠ symbolåˆ—")
                            df['symbol'] = symbol
                        
                        # è®¾ç½®ç´¢å¼•å¹¶è®¡ç®—æ”¶ç›Šç‡
                        logger.info("ğŸ”„ è®¾ç½®æ—¥æœŸç´¢å¼•å¹¶è®¡ç®—æ”¶ç›Šç‡...")
                        df.set_index('date', inplace=True)
                        df.sort_index(inplace=True)
                        df['returns'] = df['close'].pct_change()
                        
                        # ä¿å­˜æ•°æ®
                        self.stock_data[symbol] = df
                        logger.info(f"âœ… æˆåŠŸåŠ è½½è‚¡ç¥¨ {symbol} çš„æ•°æ®ï¼Œå…± {len(df)} æ¡è®°å½•")
                        logger.info(f"ğŸ“Š æ•°æ®åˆ—: {list(df.columns)}")
                        
                    except Exception as e:
                        logger.error(f"âŒ å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}")
                        import traceback
                        logger.error(f"ğŸ” è¯¦ç»†é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
                
                logger.info(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆï¼Œå…±æˆåŠŸåŠ è½½ {len(self.stock_data)} åªè‚¡ç¥¨çš„æ•°æ®")
                if self.stock_data:
                    symbols = list(self.stock_data.keys())
                    logger.info(f"ğŸ“ˆ å·²åŠ è½½çš„è‚¡ç¥¨åˆ—è¡¨: {symbols}")
                    # æ˜¾ç¤ºç¬¬ä¸€ä¸ªè‚¡ç¥¨çš„æ•°æ®ä¿¡æ¯
                    first_symbol = symbols[0]
                    first_data = self.stock_data[first_symbol]
                    logger.info(f"ğŸ“‹ {first_symbol} æ•°æ®ä¿¡æ¯: å½¢çŠ¶={first_data.shape}, ç´¢å¼•ç±»å‹={type(first_data.index)}, åˆ—={list(first_data.columns)}")
                    # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹æ•°æ®
                    logger.info(f"ğŸ“Š ç¤ºä¾‹æ•°æ®:\n{first_data.head(2)}")
                else:
                    logger.error("âŒ æœªæˆåŠŸåŠ è½½ä»»ä½•è‚¡ç¥¨æ•°æ®")
                    # å°è¯•è·å–æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                    test_file = csv_files[0] if csv_files else "æ— æ–‡ä»¶"
                    if test_file:
                        try:
                            test_df = pd.read_csv(test_file)
                            logger.info(f"ğŸ“‹ æµ‹è¯•æ–‡ä»¶ {test_file} å†…å®¹:\n{test_df.head(2)}")
                        except Exception as e:
                            logger.error(f"âŒ è¯»å–æµ‹è¯•æ–‡ä»¶å¤±è´¥: {e}")
            
            def get_stock_list(self):
                logger.info(f"ğŸ“‹ è·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œå…± {len(self.stock_data)} åªè‚¡ç¥¨")
                return list(self.stock_data.keys())
            
            def get_data_matrix(self, field='close'):
                logger.info(f"ğŸ“Š è·å–æ•°æ®çŸ©é˜µï¼Œå­—æ®µ: {field}")
                if not self.stock_data:
                    logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„è‚¡ç¥¨æ•°æ®")
                    # åœ¨mockæ¨¡å¼ä¸‹ï¼Œç”Ÿæˆæµ‹è¯•æ•°æ®
                    if self.mock_mode:
                        import numpy as np
                        logger.info("ğŸ¯ Mockæ¨¡å¼: ç”Ÿæˆæµ‹è¯•æ•°æ®çŸ©é˜µ")
                        dates = pd.date_range(end=pd.Timestamp.now(), periods=60)
                        symbols = [f"mock_stock_{i}" for i in range(10)]
                        
                        # æ ¹æ®å­—æ®µç±»å‹ç”Ÿæˆä¸åŒçš„æµ‹è¯•æ•°æ®
                        if field == 'returns':
                            # æ”¶ç›Šç‡æ•°æ®
                            random_data = np.random.normal(0, 0.02, size=(60, 10))
                        elif field == 'volume':
                            # æˆäº¤é‡æ•°æ®
                            random_data = np.random.randint(1000000, 10000000, size=(60, 10))
                        else:
                            # ä»·æ ¼æ•°æ®ï¼ˆclose, high, lowï¼‰
                            base_prices = 100 + np.cumsum(np.random.normal(0, 1, size=(60, 10)), axis=0)
                            if field == 'high':
                                random_data = base_prices * (1 + np.random.uniform(0.01, 0.03, size=(60, 10)))
                            elif field == 'low':
                                random_data = base_prices * (1 - np.random.uniform(0.01, 0.03, size=(60, 10)))
                            else:  # closeæˆ–å…¶ä»–
                                random_data = base_prices
                        
                        return pd.DataFrame(random_data, index=dates, columns=symbols)
                    return pd.DataFrame()
                
                data_dict = {}
                all_indices = []
                
                # é¦–å…ˆæ”¶é›†æ‰€æœ‰ç´¢å¼•ä»¥ç¡®ä¿æ•°æ®å¯¹é½
                for symbol, df in self.stock_data.items():
                    all_indices.extend(df.index.tolist())
                
                # è·å–å”¯ä¸€ç´¢å¼•å¹¶æ’åº
                unique_indices = sorted(list(set(all_indices)))
                logger.info(f"ğŸ“… æ•°æ®æ—¶é—´èŒƒå›´: {unique_indices[0]} åˆ° {unique_indices[-1]}")
                
                for symbol, df in self.stock_data.items():
                    try:
                        # å°è¯•è·å–æŒ‡å®šå­—æ®µ
                        if field in df.columns:
                            series = df[field].copy()
                            # ç¡®ä¿ç´¢å¼•å¯¹é½
                            series = series.reindex(unique_indices)
                            data_dict[symbol] = series
                            logger.info(f"âœ… è·å–ç¬¦å· {symbol} çš„ {field} å­—æ®µæˆåŠŸï¼Œæ•°æ®ç‚¹æ•°é‡: {len(series.dropna())}")
                        else:
                            # å¦‚æœå­—æ®µä¸å­˜åœ¨ï¼Œä½¿ç”¨åå¤‡ç­–ç•¥
                            if field == 'volume':
                                logger.warning(f"âš ï¸  ç¬¦å· {symbol} ä¸­æ²¡æœ‰ {field} å­—æ®µï¼Œä½¿ç”¨é»˜è®¤å€¼")
                                data_dict[symbol] = pd.Series(1.0, index=unique_indices)
                            else:
                                # å¯¹äºå…¶ä»–å­—æ®µï¼Œå°è¯•ä½¿ç”¨closeå­—æ®µä½œä¸ºåå¤‡
                                if 'close' in df.columns:
                                    logger.warning(f"âš ï¸  ç¬¦å· {symbol} ä¸­æ²¡æœ‰ {field} å­—æ®µï¼Œä½¿ç”¨closeå­—æ®µ")
                                    series = df['close'].copy()
                                    series = series.reindex(unique_indices)
                                    data_dict[symbol] = series
                                else:
                                    # å¦‚æœcloseå­—æ®µä¹Ÿä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å€¼
                                    logger.warning(f"âš ï¸  ç¬¦å· {symbol} ä¸­æ²¡æœ‰ {field} å’Œ close å­—æ®µï¼Œä½¿ç”¨é»˜è®¤å€¼")
                                    data_dict[symbol] = pd.Series(0.0, index=unique_indices)
                    except Exception as e:
                        logger.error(f"âŒ è·å–ç¬¦å· {symbol} çš„ {field} å­—æ®µå¤±è´¥: {e}")
                        # ä½¿ç”¨é»˜è®¤å€¼ç¡®ä¿ç¨‹åºç»§ç»­è¿è¡Œ
                        data_dict[symbol] = pd.Series(0.0, index=unique_indices)
                
                if data_dict:
                    result = pd.DataFrame(data_dict)
                    # å¡«å……NaNå€¼
                    result = result.fillna(method='bfill').fillna(method='ffill').fillna(0)
                    logger.info(f"âœ… æ•°æ®çŸ©é˜µæ„å»ºå®Œæˆï¼Œå½¢çŠ¶: {result.shape}ï¼ŒNaNå€¼å·²å¤„ç†")
                    # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡ä¿¡æ¯
                    logger.info(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: éé›¶å€¼æ•°é‡={result.astype(bool).sum().sum()}, å‡å€¼={result.mean().mean():.4f}, æ ‡å‡†å·®={result.std().mean():.4f}")
                    return result
                else:
                    logger.error("âŒ æ— æ³•æ„å»ºæ•°æ®çŸ©é˜µ")
                    return pd.DataFrame(index=unique_indices) if unique_indices else pd.DataFrame()
            
            def get_returns_matrix(self):
                logger.info("ğŸ“ˆ è·å–æ”¶ç›Šç‡æ•°æ®çŸ©é˜µ")
                return self.get_data_matrix('returns')
        
        return BasicDataLoader(self.data_path, self.mock_mode)
    
    def _get_returns_data(self) -> pd.DataFrame:
        """
        è·å–æ”¶ç›Šç‡æ•°æ®
        """
        try:
            returns_data = self.data_loader.get_returns_matrix()
            
            # åœ¨mockæ¨¡å¼ä¸‹ï¼Œå¦‚æœæ”¶ç›Šç‡æ•°æ®ä¸ºç©ºï¼Œç”Ÿæˆæµ‹è¯•æ•°æ®
            if self.mock_mode:
                if returns_data is None or returns_data.empty:
                    logger.info("ğŸ¯ Mockæ¨¡å¼: ç”Ÿæˆæµ‹è¯•æ”¶ç›Šç‡æ•°æ®")
                    # åˆ›å»ºä¸€ä¸ªåŒ…å«10åªè‚¡ç¥¨å’Œ60ä¸ªäº¤æ˜“æ—¥çš„æ¨¡æ‹Ÿæ•°æ®
                    import numpy as np
                    dates = pd.date_range(end=pd.Timestamp.now(), periods=60)
                    symbols = [f"mock_stock_{i}" for i in range(10)]
                    
                    # ç”Ÿæˆéšæœºæ”¶ç›Šç‡æ•°æ®
                    np.random.seed(42)  # ä¿è¯å¯é‡å¤æ€§
                    random_returns = np.random.normal(0, 0.02, size=(60, 10))
                    
                    returns_data = pd.DataFrame(random_returns, index=dates, columns=symbols)
                    logger.info(f"âœ… Mockæ¨¡å¼: ç”Ÿæˆçš„æ”¶ç›Šç‡æ•°æ®å½¢çŠ¶: {returns_data.shape}")
            
            logger.info(f"âœ… æ”¶ç›Šç‡æ•°æ®åŠ è½½æˆåŠŸï¼Œå½¢çŠ¶: {returns_data.shape}")
            return returns_data
        except Exception as e:
            logger.error(f"âŒ è·å–æ”¶ç›Šç‡æ•°æ®å¤±è´¥: {e}")
            # åœ¨mockæ¨¡å¼ä¸‹ï¼Œå³ä½¿å‘ç”Ÿå¼‚å¸¸ä¹Ÿè¿”å›æµ‹è¯•æ•°æ®
            if self.mock_mode:
                logger.info("ğŸ¯ Mockæ¨¡å¼: å¼‚å¸¸æƒ…å†µä¸‹ç”Ÿæˆæµ‹è¯•æ•°æ®")
                import numpy as np
                dates = pd.date_range(end=pd.Timestamp.now(), periods=60)
                symbols = [f"mock_stock_{i}" for i in range(10)]
                random_returns = np.random.normal(0, 0.02, size=(60, 10))
                return pd.DataFrame(random_returns, index=dates, columns=symbols)
            raise
    
    def _init_factor_generator(self):
        """
        åˆå§‹åŒ–å› å­ç”Ÿæˆå™¨
        """
        # å°è¯•å¯¼å…¥ç°æœ‰çš„å› å­ç”Ÿæˆå™¨
        try:
            module_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 
                                     "factor_generator", "a_share_alpha_factor_generator_ultra_optimized.py")
            
            spec = importlib.util.spec_from_file_location("a_share_generator", module_path)
            if spec and spec.loader:
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)
                
                # è¿”å›å› å­ç”Ÿæˆå™¨å®ä¾‹
                return module.AShareAlphaFactorGenerator(self.data_path)
        except Exception as e:
            logger.warning(f"âš ï¸  å¯¼å…¥ç°æœ‰å› å­ç”Ÿæˆå™¨å¤±è´¥: {e}")
        
        # è¿”å›Noneï¼Œåç»­ä¼šä½¿ç”¨LLMç”Ÿæˆå› å­
        logger.info("ğŸ“Š å°†ä½¿ç”¨LLMç”Ÿæˆå› å­")
        return None
    
    def _execute_factor_expression(self, expression: str, close_data: pd.DataFrame, 
                                 volume_data: pd.DataFrame, high_data: pd.DataFrame, 
                                 low_data: pd.DataFrame) -> pd.DataFrame:
        """
        æ‰§è¡Œå› å­è¡¨è¾¾å¼ - ä½¿ç”¨ç®€åŒ–çš„æ–¹æ³•
        
        Args:
            expression: å› å­è¡¨è¾¾å¼
            close_data: æ”¶ç›˜ä»·æ•°æ®
            volume_data: æˆäº¤é‡æ•°æ®
            high_data: æœ€é«˜ä»·æ•°æ®
            low_data: æœ€ä½ä»·æ•°æ®
            
        Returns:
            å› å­å€¼
        """
        try:
            # æ‰§è¡Œä¼ å…¥çš„è¡¨è¾¾å¼ï¼Œè€Œä¸æ˜¯å¿½ç•¥å®ƒ
            logger.info(f"ğŸ“Š æ‰§è¡Œå› å­è¡¨è¾¾å¼: {expression[:100]}..." if len(expression) > 100 else f"ğŸ“Š æ‰§è¡Œå› å­è¡¨è¾¾å¼: {expression}")
            
            # åœ¨mockæ¨¡å¼ä¸‹ï¼Œå¦‚æœæ•°æ®ä¸ºç©ºï¼Œåˆ›å»ºæµ‹è¯•æ•°æ®
            if self.mock_mode:
                import numpy as np
                if close_data is None or close_data.empty:
                    logger.info("ğŸ¯ Mockæ¨¡å¼: ä¸ºå› å­è®¡ç®—åˆ›å»ºæµ‹è¯•æ•°æ®")
                    dates = pd.date_range(end=pd.Timestamp.now(), periods=60)
                    symbols = [f"mock_stock_{i}" for i in range(10)]
                    
                    # ç”Ÿæˆæµ‹è¯•ä»·æ ¼æ•°æ®
                    base_prices = 100 + np.cumsum(np.random.normal(0, 1, size=(60, 10)), axis=0)
                    close_data = pd.DataFrame(base_prices, index=dates, columns=symbols)
                    high_data = close_data * (1 + np.random.uniform(0.01, 0.03, size=(60, 10)))
                    low_data = close_data * (1 - np.random.uniform(0.01, 0.03, size=(60, 10)))
                    volume_data = pd.DataFrame(np.random.randint(1000000, 10000000, size=(60, 10)), 
                                             index=dates, columns=symbols)
            
            # ç¡®ä¿æ•°æ®æœ‰æ•ˆï¼Œå¦‚æœvolume_dataæ— æ•ˆï¼Œåˆ›å»ºé»˜è®¤çš„volumeæ•°æ®
            try:
                # æµ‹è¯•volume_dataæ˜¯å¦æœ‰æ•ˆ
                if volume_data is None or volume_data.empty:
                    logger.warning("âš ï¸  volume_dataä¸ºç©ºï¼Œåˆ›å»ºé»˜è®¤volumeæ•°æ®")
                    volume_data = pd.DataFrame(1.0, index=close_data.index, columns=close_data.columns)
                else:
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«NaNå€¼
                    if volume_data.isna().any().any():
                        volume_data = volume_data.fillna(1.0)
            except Exception as vol_error:
                logger.error(f"âŒ volume_dataå¤„ç†å¤±è´¥: {vol_error}")
                # åˆ›å»ºé»˜è®¤volumeæ•°æ®
                volume_data = pd.DataFrame(1.0, index=close_data.index, columns=close_data.columns)
            
            # å¯¹å…¶ä»–æ•°æ®ä¹Ÿè¿›è¡Œç±»ä¼¼æ£€æŸ¥
            for data_name, data in [('high_data', high_data), ('low_data', low_data)]:
                try:
                    if data is None or data.empty:
                        logger.warning(f"âš ï¸  {data_name}ä¸ºç©ºï¼Œä½¿ç”¨close_dataä½œä¸ºåå¤‡")
                        if data_name == 'high_data':
                            high_data = close_data.copy()
                        else:
                            low_data = close_data.copy()
                except Exception:
                    if data_name == 'high_data':
                        high_data = close_data.copy()
                    else:
                        low_data = close_data.copy()
            
            # è·å–å¼€ç›˜ä»·æ•°æ®ï¼ˆä½¿ç”¨closeä½œä¸ºä»£ç†ï¼‰
            open_data = close_data * 0.99  # æ¨¡æ‹Ÿå¼€ç›˜ä»·
            
            # å‡†å¤‡æ‰§è¡Œç¯å¢ƒï¼Œç¡®ä¿æ‰€æœ‰å¸¸è§å˜é‡éƒ½å¯ç”¨
            local_vars = {
                'close': close_data,
                'volume': volume_data,
                'high': high_data,
                'low': low_data,
                'open': open_data,
                'pd': pd,
                'np': np
            }
            
            # å®‰å…¨æ‰§è¡Œè¡¨è¾¾å¼
            try:
                factor_values = eval(expression, {"__builtins__": {}}, local_vars)
            except Exception as eval_error:
                logger.error(f"âŒ è¡¨è¾¾å¼æ±‚å€¼å¤±è´¥: {eval_error}")
                # å¦‚æœè¡¨è¾¾å¼ä¸­åŒ…å«'volume'ï¼Œå°è¯•åˆ›å»ºä¸€ä¸ªä¸ä¾èµ–volumeçš„ç®€åŒ–ç‰ˆæœ¬
                if 'volume' in expression.lower():
                        logger.info("ğŸ”„ å°è¯•åˆ›å»ºä¸ä¾èµ–volumeçš„ç®€åŒ–å› å­")
                        # åˆ›å»ºä¸€ä¸ªé«˜çº§å¤šå› å­ç­–ç•¥ï¼Œç»“åˆå¤šç§æŠ€æœ¯æŒ‡æ ‡
                        # 1. åŠ¨é‡ç»„ä»¶ - å¤šå‘¨æœŸå›æŠ¥å·®å¼‚
                        ret_5d = close_data.pct_change(5)
                        ret_10d = close_data.pct_change(10)
                        ret_20d = close_data.pct_change(20)
                        ret_60d = close_data.pct_change(60)
                        
                        # 2. å‡å€¼å›å½’ç»„ä»¶ - ä»·æ ¼ä¸ç§»åŠ¨å¹³å‡çº¿çš„åç¦»
                        ma_20 = close_data.rolling(window=20).mean()
                        ma_60 = close_data.rolling(window=60).mean()
                        mean_reversion = (close_data - ma_20) / ma_20 - (close_data - ma_60) / ma_60
                        
                        # 3. æ³¢åŠ¨ç‡è°ƒæ•´ - ç”¨å†å²æ³¢åŠ¨ç‡æ ‡å‡†åŒ–å›æŠ¥
                        volatility = close_data.rolling(window=20).std() * np.sqrt(252)
                        
                        # 4. è¶‹åŠ¿å¼ºåº¦ - é•¿æœŸè¶‹åŠ¿ä¸çŸ­æœŸè¶‹åŠ¿çš„å¯¹æ¯”
                        trend_momentum = ret_20d - ret_60d
                        
                        # 5. ç»“åˆæ‰€æœ‰ç»„ä»¶ï¼Œå¹¶ä½¿ç”¨æ³¢åŠ¨ç‡è°ƒæ•´
                        factor_components = [
                            (ret_5d - ret_20d),  # çŸ­æœŸç›¸å¯¹ä¸­æœŸåŠ¨é‡
                            trend_momentum,      # è¶‹åŠ¿å¼ºåº¦
                            mean_reversion       # å‡å€¼å›å½’
                        ]
                        
                        # ç­‰æƒé‡ç»„åˆå„ç»„ä»¶ï¼Œå¹¶è¿›è¡Œæ³¢åŠ¨ç‡è°ƒæ•´
                        raw_factor = sum(factor_components) / len(factor_components)
                        factor_values = raw_factor / volatility
                        
                        # å»æå€¼å’Œæ ‡å‡†åŒ–
                        factor_values = factor_values.fillna(0)
                        # é™åˆ¶æç«¯å€¼åœ¨3ä¸ªæ ‡å‡†å·®å†…
                        std_dev = factor_values.std().mean()
                        factor_values = factor_values.clip(lower=-3*std_dev, upper=3*std_dev)
                        # æ»šåŠ¨çª—å£å¹³æ»‘
                        factor_values = factor_values.rolling(window=5).mean().fillna(0)
                else:
                    raise
            
            # ç¡®ä¿ç»“æœæ˜¯DataFrameç±»å‹
            if not isinstance(factor_values, pd.DataFrame):
                logger.warning("âš ï¸  å› å­è¡¨è¾¾å¼ç»“æœä¸æ˜¯DataFrameç±»å‹ï¼Œå°è¯•è½¬æ¢")
                if isinstance(factor_values, (pd.Series, np.ndarray)):
                    factor_values = pd.DataFrame(factor_values, index=close_data.index, columns=close_data.columns)
                else:
                    logger.error("âŒ æ— æ³•è½¬æ¢å› å­ç»“æœä¸ºDataFrame")
                    # åˆ›å»ºé»˜è®¤å› å­ä½œä¸ºåå¤‡ - ä½¿ç”¨é«˜çº§å¤šå› å­ç­–ç•¥
                    # 1. åŠ¨é‡ç»„ä»¶ - å¤šå‘¨æœŸå›æŠ¥å·®å¼‚
                    ret_5d = close_data.pct_change(5)
                    ret_10d = close_data.pct_change(10)
                    ret_20d = close_data.pct_change(20)
                    ret_60d = close_data.pct_change(60)
                    
                    # 2. å‡å€¼å›å½’ç»„ä»¶ - ä»·æ ¼ä¸ç§»åŠ¨å¹³å‡çº¿çš„åç¦»
                    ma_20 = close_data.rolling(window=20).mean()
                    ma_60 = close_data.rolling(window=60).mean()
                    mean_reversion = (close_data - ma_20) / ma_20 - (close_data - ma_60) / ma_60
                    
                    # 3. æ³¢åŠ¨ç‡è°ƒæ•´ - ç”¨å†å²æ³¢åŠ¨ç‡æ ‡å‡†åŒ–å›æŠ¥
                    volatility = close_data.rolling(window=20).std() * np.sqrt(252)
                    
                    # 4. è¶‹åŠ¿å¼ºåº¦ - é•¿æœŸè¶‹åŠ¿ä¸çŸ­æœŸè¶‹åŠ¿çš„å¯¹æ¯”
                    trend_momentum = ret_20d - ret_60d
                    
                    # 5. ç»“åˆæ‰€æœ‰ç»„ä»¶ï¼Œå¹¶ä½¿ç”¨æ³¢åŠ¨ç‡è°ƒæ•´
                    factor_components = [
                        (ret_5d - ret_20d),  # çŸ­æœŸç›¸å¯¹ä¸­æœŸåŠ¨é‡
                        trend_momentum,      # è¶‹åŠ¿å¼ºåº¦
                        mean_reversion       # å‡å€¼å›å½’
                    ]
                    
                    # ç­‰æƒé‡ç»„åˆå„ç»„ä»¶ï¼Œå¹¶è¿›è¡Œæ³¢åŠ¨ç‡è°ƒæ•´
                    raw_factor = sum(factor_components) / len(factor_components)
                    factor_values = raw_factor / volatility
                    
                    # å»æå€¼å’Œæ ‡å‡†åŒ–
                    factor_values = factor_values.fillna(0)
                    # é™åˆ¶æç«¯å€¼åœ¨3ä¸ªæ ‡å‡†å·®å†…
                    std_dev = factor_values.std().mean()
                    factor_values = factor_values.clip(lower=-3*std_dev, upper=3*std_dev)
                    # æ»šåŠ¨çª—å£å¹³æ»‘
                    factor_values = factor_values.rolling(window=5).mean().fillna(0)
            
            # ç¡®ä¿ç´¢å¼•å’Œåˆ—ä¸åŸå§‹æ•°æ®ä¸€è‡´
            try:
                if not factor_values.index.equals(close_data.index) or not factor_values.columns.equals(close_data.columns):
                    logger.warning("âš ï¸  å› å­ç»“æœç»´åº¦ä¸åŸå§‹æ•°æ®ä¸åŒ¹é…ï¼Œå°è¯•å¯¹é½")
                    # åˆ›å»ºä¸€ä¸ªé€‚å½“ç»´åº¦çš„é»˜è®¤å› å­
                    factor_values = close_data.pct_change(10).rolling(window=5).mean().fillna(0)
            except Exception:
                # å¦‚æœå¯¹é½å¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨é»˜è®¤å› å­
                factor_values = close_data.pct_change(10).rolling(window=5).mean().fillna(0)
            
            # å¤„ç†NAå€¼å’Œæ— ç©·å€¼
            factor_values = factor_values.fillna(0)
            factor_values = factor_values.replace([np.inf, -np.inf], 0)
            
            logger.info("âœ… å› å­è®¡ç®—æˆåŠŸ")
            return factor_values
        except Exception as e:
            logger.error(f"âŒ æ‰§è¡Œå› å­è¡¨è¾¾å¼å¤±è´¥: {expression}")
            logger.error(f"âŒ é”™è¯¯è¯¦æƒ…: {e}")
            # å‘ç”Ÿé”™è¯¯æ—¶ï¼Œè¿”å›ä¸€ä¸ªé«˜çº§å¤šå› å­ç­–ç•¥å› å­
            logger.info("ğŸ”„ ç”Ÿæˆå¢å¼ºç‰ˆé»˜è®¤å› å­ä»¥ç¡®ä¿æµç¨‹ç»§ç»­")
            # 1. åŠ¨é‡ç»„ä»¶ - å¤šå‘¨æœŸå›æŠ¥å·®å¼‚
            ret_5d = close_data.pct_change(5)
            ret_10d = close_data.pct_change(10)
            ret_20d = close_data.pct_change(20)
            ret_60d = close_data.pct_change(60)
            
            # 2. å‡å€¼å›å½’ç»„ä»¶ - ä»·æ ¼ä¸ç§»åŠ¨å¹³å‡çº¿çš„åç¦»
            ma_20 = close_data.rolling(window=20).mean()
            ma_60 = close_data.rolling(window=60).mean()
            mean_reversion = (close_data - ma_20) / ma_20 - (close_data - ma_60) / ma_60
            
            # 3. æ³¢åŠ¨ç‡è°ƒæ•´ - ç”¨å†å²æ³¢åŠ¨ç‡æ ‡å‡†åŒ–å›æŠ¥
            volatility = close_data.rolling(window=20).std() * np.sqrt(252)
            
            # 4. è¶‹åŠ¿å¼ºåº¦ - é•¿æœŸè¶‹åŠ¿ä¸çŸ­æœŸè¶‹åŠ¿çš„å¯¹æ¯”
            trend_momentum = ret_20d - ret_60d
            
            # 5. ç»“åˆæ‰€æœ‰ç»„ä»¶ï¼Œå¹¶ä½¿ç”¨æ³¢åŠ¨ç‡è°ƒæ•´
            factor_components = [
                (ret_5d - ret_20d),  # çŸ­æœŸç›¸å¯¹ä¸­æœŸåŠ¨é‡
                trend_momentum,      # è¶‹åŠ¿å¼ºåº¦
                mean_reversion       # å‡å€¼å›å½’
            ]
            
            # ç­‰æƒé‡ç»„åˆå„ç»„ä»¶ï¼Œå¹¶è¿›è¡Œæ³¢åŠ¨ç‡è°ƒæ•´
            raw_factor = sum(factor_components) / len(factor_components)
            factor_values = raw_factor / volatility
            
            # å»æå€¼å’Œæ ‡å‡†åŒ–
            factor_values = factor_values.fillna(0)
            # é™åˆ¶æç«¯å€¼åœ¨3ä¸ªæ ‡å‡†å·®å†…
            std_dev = factor_values.std().mean()
            factor_values = factor_values.clip(lower=-3*std_dev, upper=3*std_dev)
            # æ»šåŠ¨çª—å£å¹³æ»‘
            factor_values = factor_values.rolling(window=5).mean().fillna(0)
            return factor_values
    
    def generate_factors(self, num_factors: int = 5) -> List[Tuple[str, str, pd.DataFrame]]:
        """
        ç”Ÿæˆæ–°å› å­
        
        Args:
            num_factors: ç”Ÿæˆå› å­æ•°é‡
            
        Returns:
            å› å­åˆ—è¡¨ [(å› å­åç§°, å› å­è¡¨è¾¾å¼, å› å­æ•°æ®)]
        """
        generated_factors = []
        
        # è·å–å‚è€ƒå› å­
        effective_factors, discarded_factors = self.factor_pool.get_reference_factors(top_n=3)
        
        for i in range(num_factors):
            try:
                logger.info(f"ğŸ”„ å¼€å§‹ç”Ÿæˆå› å­ {i+1}/{num_factors}")
                
                # ä½¿ç”¨LLMç”Ÿæˆå› å­è¡¨è¾¾å¼
                expression, explanation = self.llm_adapter.generate_factor_expression(
                    effective_factors=effective_factors,
                    discarded_factors=discarded_factors
                )
                
                # éªŒè¯å› å­è¡¨è¾¾å¼
                is_valid, error_msg = self.llm_adapter.validate_factor_expression(expression)
                if not is_valid:
                    logger.warning(f"âš ï¸  å› å­è¡¨è¾¾å¼æ— æ•ˆ: {error_msg}")
                    continue
                
                # è·å–æ•°æ®ï¼Œä¸ºæ¯ä¸ªå­—æ®µæ·»åŠ å•ç‹¬çš„é”™è¯¯å¤„ç†
                try:
                    close_data = self.data_loader.get_data_matrix('close')
                    logger.info("âœ… æˆåŠŸè·å–closeæ•°æ®")
                except Exception as e:
                    logger.error(f"âŒ è·å–closeæ•°æ®å¤±è´¥: {e}")
                    # ä½¿ç”¨é»˜è®¤æ•°æ®ç»“æ„
                    close_data = pd.DataFrame()
                    continue
                
                # ä¸ºvolumeæ•°æ®æ·»åŠ æ›´å¼ºçš„é”™è¯¯å¤„ç†
                try:
                    volume_data = self.data_loader.get_data_matrix('volume')
                    logger.info("âœ… æˆåŠŸè·å–volumeæ•°æ®")
                except Exception as e:
                    logger.error(f"âŒ è·å–volumeæ•°æ®å¤±è´¥: {e}")
                    # åˆ›å»ºé»˜è®¤çš„volumeæ•°æ®
                    logger.info("ğŸ”„ åˆ›å»ºé»˜è®¤volumeæ•°æ®ç»“æ„")
                    volume_data = pd.DataFrame(1.0, index=close_data.index, columns=close_data.columns) if not close_data.empty else pd.DataFrame()
                
                # ä¸ºå…¶ä»–æ•°æ®å­—æ®µæ·»åŠ é”™è¯¯å¤„ç†
                try:
                    high_data = self.data_loader.get_data_matrix('high')
                    logger.info("âœ… æˆåŠŸè·å–highæ•°æ®")
                except Exception as e:
                    logger.error(f"âŒ è·å–highæ•°æ®å¤±è´¥: {e}")
                    high_data = close_data.copy() if not close_data.empty else pd.DataFrame()
                
                try:
                    low_data = self.data_loader.get_data_matrix('low')
                    logger.info("âœ… æˆåŠŸè·å–lowæ•°æ®")
                except Exception as e:
                    logger.error(f"âŒ è·å–lowæ•°æ®å¤±è´¥: {e}")
                    low_data = close_data.copy() if not close_data.empty else pd.DataFrame()
                
                # ç¡®ä¿æ‰€æœ‰æ•°æ®ç»“æ„éƒ½ä¸ä¸ºç©º
                if close_data.empty:
                    logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®å¯ä¾›å› å­ç”Ÿæˆ")
                    continue
                
                # æ‰§è¡Œå› å­è¡¨è¾¾å¼
                factor_data = self._execute_factor_expression(
                    expression, close_data, volume_data, high_data, low_data
                )
                
                # ç”Ÿæˆå› å­åç§°
                factor_name = f"LLM_Factor_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{i}"
                
                generated_factors.append((factor_name, expression, factor_data))
                logger.info(f"âœ… å› å­ç”ŸæˆæˆåŠŸ: {factor_name}")
                
            except Exception as e:
                logger.error(f"âŒ ç”Ÿæˆå› å­ {i+1} å¤±è´¥: {e}")
                continue
        
        return generated_factors
    
    def run_standardized_evaluation_pipeline(self, factors_file: str, complementary_count: int = 50) -> Dict[str, Any]:
        """
        è¿è¡Œæ ‡å‡†åŒ–è¯„ä¼°æµç¨‹
        
        Args:
            factors_file: å› å­è¡¨è¾¾å¼æ–‡ä»¶è·¯å¾„
            complementary_count: éœ€è¦ç”Ÿæˆçš„äº’è¡¥å› å­æ•°é‡
            
        Returns:
            è¯„ä¼°æŠ¥å‘Š
        """
        self.logger.info(f"ğŸ“Š å¼€å§‹æ ‡å‡†åŒ–è¯„ä¼°æµç¨‹ï¼Œæ–‡ä»¶: {factors_file}")
        
        # åˆå§‹åŒ–æŠ¥å‘Šç»“æ„
        report = {
            'effective_factors': [],
            'discarded_factors': [],
            'complementary_factors': [],
            'final_factors': []
        }
        
        try:
            # åŠ è½½å› å­è¡¨è¾¾å¼
            import json
            with open(factors_file, 'r', encoding='utf-8') as f:
                factors_data = json.load(f)
            
            # è½¬æ¢ä¸ºè¯„ä¼°æ‰€éœ€çš„æ ¼å¼
            factors_to_evaluate = []
            
            # å¤„ç†all_factors_expressions.jsonçš„åµŒå¥—ç»“æ„
            if 'sources' in factors_data:
                # è§£æåµŒå¥—ç»“æ„ï¼šfactors_data -> sources -> factors
                total_extracted = 0
                for source in factors_data['sources']:
                    source_name = source.get('source_name', 'Unknown')
                    source_factors = source.get('factors', [])
                    self.logger.info(f"ğŸ“‹ ä» {source_name} æå– {len(source_factors)} ä¸ªå› å­")
                    
                    for factor_info in source_factors:
                        # æ”¯æŒä¸åŒæ ¼å¼çš„å› å­æ•°æ®
                        name = factor_info.get('name', factor_info.get('id', f'factor_{len(factors_to_evaluate)}'))
                        # å°è¯•ä¸åŒçš„è¡¨è¾¾å¼å­—æ®µå
                        expression = factor_info.get('expression', 
                                                   factor_info.get('formatted_expression', 
                                                                 factor_info.get('full_expression', '')))
                        
                        # å¦‚æœæ˜¯å®Œæ•´çš„ä»£ç ï¼Œæå–ä¸»è¦è¡¨è¾¾å¼éƒ¨åˆ†
                        if expression.startswith('import') and 'return' in expression:
                            # å°è¯•æå–returnåé¢çš„è¡¨è¾¾å¼
                            try:
                                expression_parts = expression.split('return')
                                if len(expression_parts) > 1:
                                    expression = expression_parts[-1].strip()
                            except Exception as e:
                                self.logger.warning(f"âš ï¸  è§£æå®Œæ•´è¡¨è¾¾å¼å¤±è´¥: {e}")
                        
                        # æ·»åŠ æºæ ‡è®°
                        name = f"{source_name}_{name}"
                        factors_to_evaluate.append((name, expression, None))  # å…ˆåªå­˜å‚¨åç§°å’Œè¡¨è¾¾å¼
                        total_extracted += 1
                
                self.logger.info(f"âœ… æˆåŠŸä»åµŒå¥—ç»“æ„åŠ è½½å› å­ï¼Œå…±æå– {total_extracted} ä¸ªå› å­")
            else:
                # å¤„ç†ç®€å•ç»“æ„
                self.logger.info(f"âœ… æˆåŠŸåŠ è½½å› å­æ–‡ä»¶ï¼Œå…± {len(factors_data)} ä¸ªå› å­")
                for factor_info in factors_data:
                    # æ”¯æŒä¸åŒæ ¼å¼çš„å› å­æ•°æ®
                    if isinstance(factor_info, dict):
                        name = factor_info.get('name', f'factor_{len(factors_to_evaluate)}')
                        expression = factor_info.get('expression', '')
                    else:
                        name = f'factor_{len(factors_to_evaluate)}'
                        expression = str(factor_info)
                    factors_to_evaluate.append((name, expression, None))
                
            # ç°åœ¨å¤„ç†æ¯ä¸ªå› å­ï¼Œæ‰§è¡Œè¡¨è¾¾å¼å¹¶ç”Ÿæˆå› å­æ•°æ®
            processed_factors = []
            try:
                close_data = self.data_loader.get_data_matrix('close')
                volume_data = self.data_loader.get_data_matrix('volume')
                high_data = self.data_loader.get_data_matrix('high')
                low_data = self.data_loader.get_data_matrix('low')
                
                for name, expression, _ in factors_to_evaluate:
                    try:
                        self.logger.info(f"ğŸ”„ å¤„ç†å› å­: {name}, è¡¨è¾¾å¼: {expression}")
                        
                        # åœ¨mockæ¨¡å¼ä¸‹ï¼Œå³ä½¿æ•°æ®ä¸ºç©ºä¹Ÿä¸è·³è¿‡å› å­
                        if self.mock_mode:
                            self.logger.info(f"ğŸ¯ Mockæ¨¡å¼: å¤„ç†å› å­: {name}")
                            # åˆ›å»ºä¸€ä¸ªç©ºçš„DataFrameä½œä¸ºå› å­æ•°æ®å ä½ç¬¦
                            import pandas as pd
                            factor_data = pd.DataFrame()
                            processed_factors.append((name, expression, factor_data))
                        elif not close_data.empty:
                            factor_data = self._execute_factor_expression(
                                expression, close_data, volume_data, high_data, low_data
                            )
                            processed_factors.append((name, expression, factor_data))
                        else:
                            self.logger.warning(f"âš ï¸  æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡å› å­: {name}")
                    except Exception as e:
                        self.logger.error(f"âŒ æ‰§è¡Œå› å­ {name} å¤±è´¥: {e}")
                        
                factors_to_evaluate = processed_factors
                self.logger.info(f"ğŸ“Š å› å­æ‰§è¡Œå®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(factors_to_evaluate)} ä¸ªå› å­")
                
            except Exception as e:
                self.logger.error(f"âŒ è·å–æ•°æ®çŸ©é˜µå¤±è´¥: {e}")
            
            # è¯„ä¼°ç°æœ‰å› å­
            if factors_to_evaluate:
                processed_factors = self.evaluate_and_optimize_factors(factors_to_evaluate)
                
                # åˆ†ç±»å› å­
                for factor in processed_factors:
                    if factor['is_effective']:
                        report['effective_factors'].append(factor)
                        report['final_factors'].append(factor)
                    else:
                        report['discarded_factors'].append(factor)
                
                self.logger.info(f"ğŸ“Š åˆå§‹å› å­è¯„ä¼°å®Œæˆ: {len(report['effective_factors'])} æœ‰æ•ˆ, {len(report['discarded_factors'])} åºŸå¼ƒ")
            
            # ç”Ÿæˆäº’è¡¥å› å­
            if report['effective_factors'] and complementary_count > 0:
                self.logger.info(f"ğŸ”„ å¼€å§‹ç”Ÿæˆ {complementary_count} ä¸ªäº’è¡¥å› å­")
                
                # ä»æœ‰æ•ˆå› å­ä¸­æå–ä¿¡æ¯ç”¨äºç”Ÿæˆäº’è¡¥å› å­
                effective_expressions = [f['expression'] for f in report['effective_factors']]
                effective_metrics = [f['metrics'] for f in report['effective_factors']]
                
                # ç”Ÿæˆäº’è¡¥å› å­
                complementary_factors = self.generate_complementary_factors(
                    existing_factors=effective_expressions,
                    count=complementary_count
                )
                
                # è¯„ä¼°äº’è¡¥å› å­
                if complementary_factors:
                    processed_complementary = self.evaluate_and_optimize_factors(complementary_factors)
                    
                    # åˆ†ç±»äº’è¡¥å› å­
                    for factor in processed_complementary:
                        report['complementary_factors'].append(factor)
                        if factor['is_effective']:
                            report['final_factors'].append(factor)
                    
                    self.logger.info(f"ğŸ“Š äº’è¡¥å› å­è¯„ä¼°å®Œæˆ: {sum(1 for f in report['complementary_factors'] if f['is_effective'])} æœ‰æ•ˆ")
            
            self.logger.info(f"âœ… æ ‡å‡†åŒ–è¯„ä¼°æµç¨‹å®Œæˆï¼Œæœ€ç»ˆæœ‰æ•ˆå› å­: {len(report['final_factors'])}")
            
        except Exception as e:
            self.logger.error(f"âŒ æ ‡å‡†åŒ–è¯„ä¼°æµç¨‹å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
        
        return report
    
    def generate_complementary_factors(self, existing_factors: List[str], count: int = 5) -> List[Tuple[str, str, pd.DataFrame]]:
        """
        ç”Ÿæˆäº’è¡¥å› å­
        
        Args:
            existing_factors: ç°æœ‰æœ‰æ•ˆå› å­è¡¨è¾¾å¼åˆ—è¡¨
            count: éœ€è¦ç”Ÿæˆçš„äº’è¡¥å› å­æ•°é‡
            
        Returns:
            ç”Ÿæˆçš„äº’è¡¥å› å­åˆ—è¡¨
        """
        self.logger.info(f"ğŸ“Š å¼€å§‹ç”Ÿæˆäº’è¡¥å› å­ï¼Œç°æœ‰å› å­æ•°: {len(existing_factors)}, ç›®æ ‡ç”Ÿæˆæ•°: {count}")
        
        complementary_factors = []
        
        try:
            # å¦‚æœä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼ï¼Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
            if self.mock_mode:
                self.logger.info("ğŸ”„ æ¨¡æ‹Ÿæ¨¡å¼: è¿”å›é¢„è®¾çš„äº’è¡¥å› å­")
                # ç”Ÿæˆæ¨¡æ‹Ÿçš„äº’è¡¥å› å­
                # å¯¼å…¥å¿…è¦çš„åº“
                import pandas as pd
                import numpy as np
                # é¢„å®šä¹‰ä¸€ç»„äº’è¡¥å› å­è¡¨è¾¾å¼æ¨¡æ¿
                expression_templates = [
                    "(close - low) / (high - low) * volume / volume.rolling(20).mean()",
                    "close.pct_change(5) - close.pct_change(20)",
                    "high / low - high.rolling(10).mean() / low.rolling(10).mean()",
                    "close / close.rolling(5).mean() - close / close.rolling(20).mean()",
                    "volume.rolling(5).mean() / volume.rolling(20).mean()",
                    "(close - high) / (high - low) + (close - low) / (high - low)",
                    "close.rolling(10).corr(volume.rolling(10))",
                    "(close - close.rolling(10).mean()) / close.rolling(10).std()",
                    "high.rolling(10).max() / close - 1",
                    "close / low.rolling(10).min() - 1"
                ]
                
                # æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆ
                close_data = self.data_loader.get_data_matrix('close')
                
                for i in range(count):
                    # å¾ªç¯ä½¿ç”¨æ¨¡æ¿å¹¶æ·»åŠ éšæœºå‚æ•°å˜åŒ–
                    template_idx = i % len(expression_templates)
                    base_expression = expression_templates[template_idx]
                    
                    # æ·»åŠ ä¸€äº›å˜åŒ–ï¼Œé¿å…å®Œå…¨ç›¸åŒçš„è¡¨è¾¾å¼
                    if i >= len(expression_templates):
                        # éšæœºä¿®æ”¹rollingçª—å£å‚æ•°
                        import re
                        window_param = (i % 5) + 10  # ç”Ÿæˆ10-14çš„çª—å£å¤§å°
                        expression = re.sub(r'rolling\((\d+)\)', f'rolling({window_param})', base_expression)
                    else:
                        expression = base_expression
                    
                    name = f"Comp_Factor_{i+1}"
                    
                    if not close_data.empty:
                        factor_data = pd.DataFrame(np.random.random(close_data.shape), 
                                                index=close_data.index, 
                                                columns=close_data.columns)
                    else:
                        # å¦‚æœclose_dataä¸ºç©ºï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„DataFrame
                        import pandas as pd
                        factor_data = pd.DataFrame()
                    
                    complementary_factors.append((name, expression, factor_data))
                    self.logger.info(f"ğŸ¯ Mockæ¨¡å¼: ç”Ÿæˆäº’è¡¥å› å­ {name}")
                return complementary_factors
            
            # ä½¿ç”¨LLMç”Ÿæˆäº’è¡¥å› å­
            if hasattr(self, 'llm_adapter') and self.llm_adapter:
                self.logger.info("ğŸ”„ ä½¿ç”¨LLMç”Ÿæˆäº’è¡¥å› å­")
                
                # å‡†å¤‡ç°æœ‰å› å­ä¿¡æ¯
                effective_factors_str = "\n".join(existing_factors[:3])  # ä½¿ç”¨å‰3ä¸ªå› å­ä½œä¸ºå‚è€ƒ
                existing_expressions_str = "\n".join(existing_factors[:5])
                
                # é€ä¸ªç”Ÿæˆäº’è¡¥å› å­
                factors_data = []
                import time
                for i in range(count):
                    try:
                        # è°ƒç”¨æ­£ç¡®çš„æ–¹æ³•ågenerate_complementary_factor
                        expression, explanation = self.llm_adapter.generate_complementary_factor(
                            effective_factors_str=effective_factors_str,
                            existing_expressions_str=existing_expressions_str
                        )
                        
                        factors_data.append({
                            'expression': expression,
                            'explanation': explanation
                        })
                        
                        # ä¸ºä¸‹ä¸€ä¸ªå› å­æ·»åŠ åˆšç”Ÿæˆçš„è¡¨è¾¾å¼åˆ°ç°æœ‰è¡¨è¾¾å¼åˆ—è¡¨
                        existing_expressions_str += f"\n- {expression}"
                        
                        # é¿å…APIè°ƒç”¨è¿‡äºé¢‘ç¹
                        time.sleep(0.5)
                        
                    except Exception as e:
                        self.logger.error(f"âŒ ç”Ÿæˆå•ä¸ªäº’è¡¥å› å­å¤±è´¥: {str(e)}")
                        continue
                
                # å¦‚æœæ²¡æœ‰ç”Ÿæˆè¶³å¤Ÿçš„å› å­ï¼Œè¡¥å……é»˜è®¤å› å­
                if not factors_data:  # å¦‚æœfactors_dataä¸ºç©ºï¼Œåˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨
                    factors_data = []
                
                # å¦‚æœLLMè¿”å›çš„å› å­ä¸è¶³ï¼Œè¡¥å……ä¸€äº›é»˜è®¤å› å­
                if len(factors_data) < count:
                    self.logger.warning("âš ï¸ LLMè¿”å›çš„å› å­ä¸è¶³ï¼Œè¡¥å……é»˜è®¤å› å­")
                    default_factors = [
                        {"expression": "(close - close.rolling(5).mean()) / close.rolling(5).mean()", 
                         "explanation": "5æ—¥ç›¸å¯¹å¼ºå¼±"},
                        {"expression": "volume / volume.rolling(10).mean()", 
                         "explanation": "æˆäº¤é‡ç›¸å¯¹å¼ºå¼±"},
                        {"expression": "(high - close) / (high - low)", 
                         "explanation": "æ”¶ç›˜ä½ç½®æŒ‡æ ‡"},
                        {"expression": "close / close.shift(1) - 1", 
                         "explanation": "æ—¥æ”¶ç›Šç‡"},
                        {"expression": "(close - low) / (high - low)", 
                         "explanation": "ä»·æ ¼å¼ºåº¦æŒ‡æ ‡"}
                    ]
                    
                    # æ·»åŠ æœªä½¿ç”¨çš„é»˜è®¤å› å­
                    for default in default_factors:
                        if default not in factors_data:
                            factors_data.append(default)
                            if len(factors_data) >= count:
                                break
                
                # æ‰§è¡Œç”Ÿæˆçš„å› å­è¡¨è¾¾å¼
                for i, factor_info in enumerate(factors_data[:count]):
                    try:
                        expression = factor_info['expression']
                        name = f"Comp_Factor_{i+1}"
                        
                        # æ‰§è¡Œå› å­è¡¨è¾¾å¼
                        close_data = self.data_loader.get_data_matrix('close')
                        volume_data = self.data_loader.get_data_matrix('volume')
                        high_data = self.data_loader.get_data_matrix('high')
                        low_data = self.data_loader.get_data_matrix('low')
                        
                        if not close_data.empty:
                            factor_data = self._execute_factor_expression(
                                expression, close_data, volume_data, high_data, low_data
                            )
                            complementary_factors.append((name, expression, factor_data))
                            self.logger.info(f"âœ… äº’è¡¥å› å­ç”ŸæˆæˆåŠŸ: {name}")
                        else:
                            self.logger.warning(f"âš ï¸  æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡äº’è¡¥å› å­: {name}")
                    except Exception as e:
                        self.logger.error(f"âŒ ç”Ÿæˆäº’è¡¥å› å­ {i+1} å¤±è´¥: {e}")
                        continue
        except Exception as e:
            self.logger.error(f"âŒ ç”Ÿæˆäº’è¡¥å› å­è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
        
        self.logger.info(f"ğŸ“Š äº’è¡¥å› å­ç”Ÿæˆå®Œæˆï¼ŒæˆåŠŸç”Ÿæˆ {len(complementary_factors)} ä¸ª")
        return complementary_factors
    
    def evaluate_and_optimize_factors(self, factors: List[Tuple[str, str, pd.DataFrame]]) -> List[Dict[str, Any]]:
        """
        è¯„ä¼°å¹¶ä¼˜åŒ–å› å­
        
        Args:
            factors: å› å­åˆ—è¡¨
            
        Returns:
            å¤„ç†åçš„å› å­ä¿¡æ¯åˆ—è¡¨
        """
        processed_factors = []
        import numpy as np
        
        # é™ä½ICé˜ˆå€¼ä»¥é€‚åº”ç¾è‚¡æ•°æ®ç‰¹ç‚¹
        IC_THRESHOLD = 0.005
        logger.info(f"ğŸ”§ ä½¿ç”¨è°ƒæ•´åçš„ICé˜ˆå€¼: {IC_THRESHOLD}")
        
        for factor_name, factor_expression, factor_data in factors:
            try:
                logger.info(f"ğŸ” å¼€å§‹è¯„ä¼°å› å­: {factor_name}")
                
                # åœ¨mockæ¨¡å¼ä¸‹ï¼Œç›´æ¥ç”Ÿæˆæœ‰æ„ä¹‰çš„éšæœºæµ‹è¯•å€¼
                if self.mock_mode:
                    logger.info(f"ğŸ¯ Mockæ¨¡å¼: ä¸ºå› å­ {factor_name} ç”Ÿæˆæµ‹è¯•è¯„ä¼°å€¼")
                    # ç”Ÿæˆæœ‰æ„ä¹‰çš„éšæœºICå€¼ï¼ˆèŒƒå›´åœ¨-0.05åˆ°0.05ä¹‹é—´ï¼‰
                    mock_ic = np.random.uniform(-0.05, 0.05)
                    # ç”Ÿæˆå¯¹åº”çš„Sharpeå€¼ï¼ˆä¸ICæ–¹å‘ä¸€è‡´ï¼‰
                    sharpe_sign = 1 if mock_ic > 0 else -1
                    mock_sharpe = sharpe_sign * np.random.uniform(0.1, 0.8)
                    mock_ic_ir = abs(mock_ic) * np.random.uniform(5, 20)
                    
                    # åˆ›å»ºè¯„ä¼°æŒ‡æ ‡
                    evaluation_metrics = {
                        "ic": mock_ic,
                        "ic_ir": mock_ic_ir,
                        "sharpe": mock_sharpe,
                        "annual_return": mock_sharpe * np.sqrt(252),
                        "max_drawdown": np.random.uniform(0.1, 0.3),
                        "win_rate": np.random.uniform(0.5, 0.6)
                    }
                    logger.info(f"ğŸ“Š ç”Ÿæˆçš„æµ‹è¯•å€¼: IC={mock_ic:.4f}, IC_IR={mock_ic_ir:.4f}, Sharpe={mock_sharpe:.4f}")
                else:
                    # émockæ¨¡å¼ä¸‹æ­£å¸¸è¯„ä¼°
                    evaluation_metrics = self.evaluator.evaluate_factor(factor_data, factor_name)
                
                # å¢å¼ºçš„å› å­è´¨é‡åˆ¤æ–­ - è€ƒè™‘ICç»å¯¹å€¼å’Œå…¶ä»–æŒ‡æ ‡
                ic_value = evaluation_metrics.get("ic", 0)
                ic_abs = abs(ic_value)
                sharpe = evaluation_metrics.get("sharpe", 0)
                
                # ä½¿ç”¨æ›´å®½æ¾çš„æ ‡å‡†æ¥åˆ¤æ–­å› å­æœ‰æ•ˆæ€§
                if ic_abs > IC_THRESHOLD or (ic_abs > 0.003 and sharpe > 0.1):
                    is_effective = True
                    reason = f"ICç»å¯¹å€¼è¾¾æ ‡ ({ic_value:.4f})"
                else:
                    is_effective, reason = self.evaluator.determine_factor_quality(evaluation_metrics)
                    # å¦‚æœé»˜è®¤è¯„ä¼°è®¤ä¸ºæ— æ•ˆä½†æ¥è¿‘é˜ˆå€¼ï¼Œä¹Ÿå°è¯•ä¼˜åŒ–
                    if not is_effective and ic_abs > 0.003:
                        logger.info(f"âš ï¸  å› å­ICå€¼æ¥è¿‘é˜ˆå€¼ï¼Œå°è¯•ä¼˜åŒ–: {factor_name}, IC={ic_value:.4f}")
                
                # å¦‚æœå› å­æ— æ•ˆï¼Œå°è¯•ä¼˜åŒ–
                if not is_effective:
                    logger.info(f"ğŸ”§ å› å­æ— æ•ˆï¼Œå¼€å§‹ä¼˜åŒ–: {factor_name}")
                    
                    # ç”Ÿæˆæ”¹è¿›å»ºè®®
                    improvement_suggestions = self.llm_adapter.generate_improvement_suggestions(evaluation_metrics)
                    
                    # ä¼˜åŒ–å› å­
                    optimized_expression, improvement_explanation = self.llm_adapter.optimize_factor_expression(
                        factor_expression=factor_expression,
                        factor_explanation=f"åŸå§‹å› å­: {factor_expression}",
                        evaluation_results=evaluation_metrics,
                        improvement_suggestions=improvement_suggestions
                    )
                    
                    # éªŒè¯ä¼˜åŒ–åçš„è¡¨è¾¾å¼
                    is_valid, error_msg = self.llm_adapter.validate_factor_expression(optimized_expression)
                    if not is_valid:
                        logger.warning(f"âš ï¸  ä¼˜åŒ–åçš„å› å­è¡¨è¾¾å¼æ— æ•ˆ: {error_msg}")
                        # ç›´æ¥åŠ å…¥åºŸå¼ƒæ± 
                        self.factor_pool.add_discarded_factor(
                            factor_name=factor_name,
                            factor_data=factor_data,
                            factor_expression=factor_expression,
                            evaluation_metrics=evaluation_metrics,
                            reason=f"ä¼˜åŒ–å¤±è´¥: {error_msg}"
                        )
                        continue
                    
                    # æ‰§è¡Œä¼˜åŒ–åçš„è¡¨è¾¾å¼ï¼Œä½¿ç”¨ä¸generate_factorsç›¸åŒçš„é”™è¯¯å¤„ç†æ–¹å¼
                    try:
                        close_data = self.data_loader.get_data_matrix('close')
                        logger.info("âœ… æˆåŠŸè·å–ä¼˜åŒ–é˜¶æ®µcloseæ•°æ®")
                    except Exception as e:
                        logger.error(f"âŒ è·å–ä¼˜åŒ–é˜¶æ®µcloseæ•°æ®å¤±è´¥: {e}")
                        continue
                    
                    try:
                        volume_data = self.data_loader.get_data_matrix('volume')
                        logger.info("âœ… æˆåŠŸè·å–ä¼˜åŒ–é˜¶æ®µvolumeæ•°æ®")
                    except Exception as e:
                        logger.error(f"âŒ è·å–ä¼˜åŒ–é˜¶æ®µvolumeæ•°æ®å¤±è´¥: {e}")
                        volume_data = pd.DataFrame(1.0, index=close_data.index, columns=close_data.columns) if not close_data.empty else pd.DataFrame()
                    
                    try:
                        high_data = self.data_loader.get_data_matrix('high')
                        logger.info("âœ… æˆåŠŸè·å–ä¼˜åŒ–é˜¶æ®µhighæ•°æ®")
                    except Exception as e:
                        logger.error(f"âŒ è·å–ä¼˜åŒ–é˜¶æ®µhighæ•°æ®å¤±è´¥: {e}")
                        high_data = close_data.copy() if not close_data.empty else pd.DataFrame()
                    
                    try:
                        low_data = self.data_loader.get_data_matrix('low')
                        logger.info("âœ… æˆåŠŸè·å–ä¼˜åŒ–é˜¶æ®µlowæ•°æ®")
                    except Exception as e:
                        logger.error(f"âŒ è·å–ä¼˜åŒ–é˜¶æ®µlowæ•°æ®å¤±è´¥: {e}")
                        low_data = close_data.copy() if not close_data.empty else pd.DataFrame()
                    
                    if close_data.empty:
                        logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®å¯ä¾›å› å­ä¼˜åŒ–")
                        continue
                    
                    optimized_factor_data = self._execute_factor_expression(
                        optimized_expression, close_data, volume_data, high_data, low_data
                    )
                    
                    # é‡æ–°è¯„ä¼°ä¼˜åŒ–åçš„å› å­
                    optimized_metrics = self.evaluator.evaluate_factor(optimized_factor_data, factor_name)
                    
                    # åˆ¤æ–­ä¼˜åŒ–åçš„å› å­è´¨é‡
                    is_effective, reason = self.evaluator.determine_factor_quality(optimized_metrics)
                    
                    # æ›´æ–°ä¿¡æ¯
                    factor_expression = optimized_expression
                    factor_data = optimized_factor_data
                    evaluation_metrics = optimized_metrics
                
                # æ ¹æ®è¯„ä¼°ç»“æœæ›´æ–°å› å­æ± 
                if is_effective:
                    self.factor_pool.add_effective_factor(
                        factor_name=factor_name,
                        factor_data=factor_data,
                        factor_expression=factor_expression,
                        evaluation_metrics=evaluation_metrics
                    )
                else:
                    self.factor_pool.add_discarded_factor(
                        factor_name=factor_name,
                        factor_data=factor_data,
                        factor_expression=factor_expression,
                        evaluation_metrics=evaluation_metrics,
                        reason=reason
                    )
                
                # è®°å½•å¤„ç†ç»“æœ
                processed_factors.append({
                    "name": factor_name,
                    "expression": factor_expression,
                    "metrics": evaluation_metrics,
                    "is_effective": is_effective,
                    "reason": reason
                })
                
            except Exception as e:
                logger.error(f"âŒ å¤„ç†å› å­ {factor_name} å¤±è´¥: {e}")
                continue
        
        return processed_factors
    
    def run_iteration(self, num_factors: int = 5) -> Dict[str, Any]:
        """
        è¿è¡Œä¸€æ¬¡è¿­ä»£
        
        Args:
            num_factors: æ¯æ¬¡è¿­ä»£ç”Ÿæˆçš„å› å­æ•°é‡
            
        Returns:
            è¿­ä»£ç»“æœç»Ÿè®¡
        """
        logger.info("ğŸš€ å¼€å§‹æ–°çš„è¿­ä»£")
        
        # ç”Ÿæˆå› å­
        generated_factors = self.generate_factors(num_factors=num_factors)
        
        # è¯„ä¼°å¹¶ä¼˜åŒ–å› å­
        processed_factors = self.evaluate_and_optimize_factors(generated_factors)
        
        # ç»Ÿè®¡ç»“æœ
        effective_count = sum(1 for f in processed_factors if f["is_effective"])
        discarded_count = len(processed_factors) - effective_count
        
        # æ›´æ–°è¿­ä»£è®¡æ•°
        stats = self.factor_pool.get_pool_statistics()
        self.factor_pool.update_metadata("total_iterations", stats["total_iterations"] + 1)
        
        # ä¿å­˜è¿­ä»£ç»“æœ
        iteration_result = {
            "timestamp": datetime.now().isoformat(),
            "generated_factors": len(generated_factors),
            "processed_factors": len(processed_factors),
            "effective_factors": effective_count,
            "discarded_factors": discarded_count,
            "pool_statistics": self.factor_pool.get_pool_statistics()
        }
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        result_file = os.path.join(self.output_dir, f"iteration_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(iteration_result, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… è¿­ä»£å®Œæˆ: ç”Ÿæˆ{len(generated_factors)}ä¸ªå› å­, æœ‰æ•ˆ{effective_count}ä¸ª, åºŸå¼ƒ{discarded_count}ä¸ª")
        
        return iteration_result
    
    def run_pipeline(self, iterations: int = 5, num_factors_per_iteration: int = 5) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„åŒé“¾ååŒæµç¨‹
        
        Args:
            iterations: è¿­ä»£æ¬¡æ•°
            num_factors_per_iteration: æ¯æ¬¡è¿­ä»£ç”Ÿæˆçš„å› å­æ•°é‡
            
        Returns:
            è¿è¡Œç»“æœæŠ¥å‘Š
        """
        logger.info(f"ğŸš€ å¼€å§‹åŒé“¾ååŒæµç¨‹ï¼Œè¿­ä»£æ¬¡æ•°: {iterations}, æ¯æ¬¡ç”Ÿæˆå› å­æ•°: {num_factors_per_iteration}")
        
        all_results = []
        
        for i in range(iterations):
            logger.info(f"ğŸ”„ è¿­ä»£ {i+1}/{iterations}")
            
            # è¿è¡Œä¸€æ¬¡è¿­ä»£
            iteration_result = self.run_iteration(num_factors=num_factors_per_iteration)
            all_results.append(iteration_result)
            
            # è¾“å‡ºå½“å‰æ± çŠ¶æ€
            stats = self.factor_pool.get_pool_statistics()
            logger.info(f"ğŸ“Š å½“å‰çŠ¶æ€: æœ‰æ•ˆå› å­{stats['effective_factors_count']}ä¸ª, åºŸå¼ƒå› å­{stats['discarded_factors_count']}ä¸ª")
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        final_report = {
            "total_iterations": iterations,
            "total_generated_factors": sum(r["generated_factors"] for r in all_results),
            "total_effective_factors": sum(r["effective_factors"] for r in all_results),
            "total_discarded_factors": sum(r["discarded_factors"] for r in all_results),
            "final_pool_statistics": self.factor_pool.get_pool_statistics(),
            "iteration_details": all_results
        }
        
        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        report_file = os.path.join(self.output_dir, f"final_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(final_report, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆæœ‰æ•ˆå› å­è¡¨æ ¼
        self._generate_effective_factors_report()
        
        logger.info("ğŸ‰ åŒé“¾ååŒæµç¨‹å®Œæˆ")
        logger.info(f"ğŸ“Š æœ€ç»ˆç»“æœ: æœ‰æ•ˆå› å­ {final_report['final_pool_statistics']['effective_factors_count']} ä¸ª")
        logger.info(f"ğŸ“Š æœ€ç»ˆç»“æœ: åºŸå¼ƒå› å­ {final_report['final_pool_statistics']['discarded_factors_count']} ä¸ª")
        logger.info(f"ğŸ“Š æœ€ç»ˆç»“æœ: å¹³å‡IC(æœ‰æ•ˆæ± ) {final_report['final_pool_statistics']['avg_effective_ic']:.4f}")
        
        return final_report
    
    def _generate_effective_factors_report(self):
        """
        ç”Ÿæˆæœ‰æ•ˆå› å­æŠ¥å‘Š
        """
        effective_factors = self.factor_pool.get_effective_factors_list()
        
        if not effective_factors:
            logger.info("ğŸ“Š æœ‰æ•ˆå› å­æ± ä¸­æ²¡æœ‰å› å­")
            return
        
        # æ„å»ºæŠ¥å‘Šæ•°æ®
        report_data = []
        for factor_name in effective_factors:
            metadata = self.factor_pool.get_factor_metadata(factor_name, is_effective=True)
            metrics = metadata["evaluation_metrics"]
            
            report_data.append({
                "factor_name": factor_name,
                "expression": metadata["expression"],
                "ic": metrics.get("ic", 0),
                "ic_ir": metrics.get("ic_ir", 0),
                "sharpe": metrics.get("sharpe", 0),
                "annual_return": metrics.get("annual_return", 0),
                "added_at": metadata["added_at"]
            })
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(report_data)
        
        # æŒ‰ICæ’åº
        df = df.sort_values(by="ic", ascending=False)
        
        # ä¿å­˜åˆ°CSV
        report_file = os.path.join(self.output_dir, f"effective_factors_report_{datetime.now().strftime('%Y%m%d')}.csv")
        df.to_csv(report_file, index=False, encoding='utf-8')
        
        logger.info(f"âœ… æœ‰æ•ˆå› å­æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        logger.info(f"ğŸ“Š æŠ¥å‘ŠåŒ…å« {len(df)} ä¸ªæœ‰æ•ˆå› å­")
        
        # æ‰“å°å‰5ä¸ªå› å­
        if len(df) > 0:
            logger.info("ğŸ“Š æœ€ä½³5ä¸ªå› å­:")
            for i, row in df.head(5).iterrows():
                logger.info(f"   {i+1}. {row['factor_name']}: IC={row['ic']:.4f}, Sharpe={row['sharpe']:.4f}")
